{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Competition Link: https://datahack.analyticsvidhya.com/contest/black-friday/\n",
    "\n",
    "A retail company “ABC Private Limited” wants to understand the customer purchase behaviour (specifically, purchase amount) against various products of different categories. They have shared purchase summary of various customers for selected high volume products from last month.\n",
    "The data set also contains customer demographics (age, gender, marital status, city_type, stay_in_current_city), product details (product_id and product category) and Total purchase_amount from last month.\n",
    "\n",
    "Now, they want to build a model to predict the purchase amount of customer against various products which will help them to create personalized offer for customers against different products.\n",
    "\n",
    "Your model performance will be evaluated on the basis of your prediction of the purchase amount for the test data (test.csv), which contains similar data-points as train except for their purchase amount. Your submission needs to be in the format as shown in \"SampleSubmission.csv\".\n",
    "\n",
    "We at our end, have the actual purchase amount for the test dataset, against which your predictions will be evaluated. Submissions are scored on the root mean squared error (RMSE)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Necessary imports for viewing data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading test and train data set\n",
    "train_df = pd.read_csv(r'train.csv')\n",
    "test_df = pd.read_csv(r'test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's have a look at how the data looks using the head\n",
    "train_df.head()\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 12 columns:\n",
    "User_ID, Product_ID, Gender, Age, Occupation, City_Category, Stay_In_Current_City_Years, Marital_Status, Product_Category_1, Product_Category_2, Product_Category_3, Purchase\n",
    "\n",
    "Target column is Purchase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check for columns with null values\n",
    "print(\"Train Data***********\")\n",
    "print(train_df.isnull().mean() * 100)\n",
    "print(\"Test Data***********\")\n",
    "print(test_df.isnull().mean() * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that Product_Category_2 and Product_Category_3 have null values\n",
    "train_df has 31% and 70% null values in Product_Category_2 and Product_Category_3 respectively\n",
    "test_df has 31% and 70% null values in Product_Category_2 and Product_Category_3 respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#All the columns are categorical. Let's check unique number of entries in each column for Training set\n",
    "print(\"User_ID: \"+str(train_df[\"User_ID\"].unique().shape[0]))\n",
    "print(\"Product_ID: \"+str(train_df[\"Product_ID\"].unique().shape[0]))\n",
    "print(\"Gender: \"+str(train_df[\"Gender\"].unique().shape[0]))\n",
    "print(\"Age: \"+str(train_df[\"Age\"].unique().shape[0]))\n",
    "print(\"Occupation: \"+str(train_df[\"Occupation\"].unique().shape[0]))\n",
    "print(\"City_Category: \"+str(train_df[\"City_Category\"].unique().shape[0]))\n",
    "print(\"Stay_In_Current_City_Years: \"+str(train_df[\"Stay_In_Current_City_Years\"].unique().shape[0]))\n",
    "print(\"Marital_Status: \"+str(train_df[\"Marital_Status\"].unique().shape[0]))\n",
    "print(\"Product_Category_1: \"+str(train_df[\"Product_Category_1\"].unique().shape[0]))\n",
    "print(\"Product_Category_2: \"+str(train_df[\"Product_Category_2\"].unique().shape[0]))\n",
    "print(\"Product_Category_3: \"+str(train_df[\"Product_Category_3\"].unique().shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's check unique number of entries in each column for Test set\n",
    "print(\"User_ID: \"+str(test_df[\"User_ID\"].unique().shape[0]))\n",
    "print(\"Product_ID: \"+str(test_df[\"Product_ID\"].unique().shape[0]))\n",
    "print(\"Gender: \"+str(test_df[\"Gender\"].unique().shape[0]))\n",
    "print(\"Age: \"+str(test_df[\"Age\"].unique().shape[0]))\n",
    "print(\"Occupation: \"+str(test_df[\"Occupation\"].unique().shape[0]))\n",
    "print(\"City_Category: \"+str(test_df[\"City_Category\"].unique().shape[0]))\n",
    "print(\"Stay_In_Current_City_Years: \"+str(test_df[\"Stay_In_Current_City_Years\"].unique().shape[0]))\n",
    "print(\"Marital_Status: \"+str(test_df[\"Marital_Status\"].unique().shape[0]))\n",
    "print(\"Product_Category_1: \"+str(test_df[\"Product_Category_1\"].unique().shape[0]))\n",
    "print(\"Product_Category_2: \"+str(test_df[\"Product_Category_2\"].unique().shape[0]))\n",
    "print(\"Product_Category_3: \"+str(test_df[\"Product_Category_3\"].unique().shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test set has 140 less product ids and 2 less product categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_df['Product_Category_1'].unique())\n",
    "print(test_df['Product_Category_1'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that 19 and 20 are not there in Product_Category_1 for test data. I want to check if the 140 extra product ids in the training data are from the 2 product categories (19 and 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missed_values = train_df.loc[~train_df['Product_ID'].isin(test_df[\"Product_ID\"].unique())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missed_values[\"Product_Category_1\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "so, the 140 extra product ids are not from the product categories 19 and 20.\n",
    "Lets look at the frequency of occurence of user id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['User_ID'].value_counts().describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['User_ID'].value_counts().head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['User_ID'].value_counts(ascending = True).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets see if the value counts have any effect on the purchase. \n",
    "Add the user_id_count as a column to the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_id_grouped = train_df.groupby(\"User_ID\")\n",
    "count_dict = {}\n",
    "for name, group in user_id_grouped:\n",
    "    count_dict[name] = group.shape[0]\n",
    "count_list = []\n",
    "for index, row in train_df.iterrows():\n",
    "    name = row[\"User_ID\"]\n",
    "    count_list.append(count_dict.get(name, 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the correlation of this new field with the purchase "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[\"User_ID_Count\"] = count_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['User_ID_Count'].corr(train_df['Purchase'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The user id count is correlated to the Purchase.Let's look at the count values of other variables, and for that we need to convert the categorical variables into numbers. They are:\n",
    "<br> Gender\n",
    "<br> Age\n",
    "<br> City_Category\n",
    "<br> Stay_In_Current_City\n",
    "<br> Define dictionaries to convert the categorical features into numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_dict = {'F':0, 'M':1}\n",
    "age_dict = {'0-17':0, '18-25':1, '26-35':2, '36-45':3, '46-50':4, '51-55':5, '55+':6}\n",
    "city_dict = {'A':0, 'B':1, 'C':2}\n",
    "stay_dict = {'0':0, '1':1, '2':2, '3':3, '4+':4}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the dictionary above to convert the categorical variables into numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[\"Gender\"] = train_df[\"Gender\"].apply(lambda x: gender_dict[x])\n",
    "train_df[\"Age\"] = train_df[\"Age\"].apply(lambda x: age_dict[x])\n",
    "train_df[\"City_Category\"] = train_df[\"City_Category\"].apply(lambda x: city_dict[x])\n",
    "train_df[\"Stay_In_Current_City_Years\"] = train_df[\"Stay_In_Current_City_Years\"].apply(lambda x: stay_dict[x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have  alook at the variables now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create a function to give the count of the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCountofVar(dataset_df, var_name):\n",
    "    var_name_grouped = dataset_df.groupby(var_name)\n",
    "    count_dict = {}\n",
    "    for name, group in var_name_grouped:\n",
    "        count_dict[name] = group.shape[0]\n",
    "    count_list = []\n",
    "    for index, row in dataset_df.iterrows():\n",
    "        name = row[var_name]\n",
    "        count_list.append(count_dict.get(name, 0))\n",
    "    return count_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get counts for all other features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[\"Product_ID_Count\"] = getCountofVar(train_df,\"Product_ID\")\n",
    "train_df[\"Gender_Count\"] = getCountofVar(train_df,\"Gender\")\n",
    "train_df[\"Age_Count\"] = getCountofVar(train_df,\"Age\")\n",
    "train_df[\"Occupation_Count\"] = getCountofVar(train_df,\"Occupation\")\n",
    "train_df[\"City_Count\"] = getCountofVar(train_df,\"City_Category\")\n",
    "train_df[\"Stay_Count\"] = getCountofVar(train_df,\"Stay_In_Current_City_Years\")\n",
    "train_df[\"Marital_Status_Count\"] = getCountofVar(train_df,\"Marital_Status\")\n",
    "train_df[\"PC1_Count\"] = getCountofVar(train_df,\"Product_Category_1\")\n",
    "train_df[\"PC2_Count\"] = getCountofVar(train_df,\"Product_Category_2\")\n",
    "train_df[\"PC3_Count\"] = getCountofVar(train_df,\"Product_Category_3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Look at the columns once\n",
    "train_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets look at the correlation of the features\n",
    "corr = train_df.corr()\n",
    "corr.style.background_gradient(cmap='coolwarm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#correlation of all other columns with Purchase\n",
    "train_df[train_df.columns[1:]].corr()['Purchase'][:]\n",
    "#to see the correlation in a csv file\n",
    "#corr.to_csv(r'corr_table.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I want to check if the minimum values, max values, mean of the features have any effect on the purchase.\n",
    "<br> But, before this I will impute the missing values with 0. I did not impute the missing values earlier as it will cause the counts to reflect values. Right now the count for missing values is 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For every user, we find the min purchase, max purchase and mean purchase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_user_id_grouped = train_df.groupby(\"User_ID\")\n",
    "min_dict = {}\n",
    "max_dict = {}\n",
    "mean_dict = {}\n",
    "for name, group in train_df_user_id_grouped:\n",
    "    min_dict[name] = min(np.array(group[\"Purchase\"]))\n",
    "    max_dict[name] = max(np.array(group[\"Purchase\"]))\n",
    "    mean_dict[name] = np.mean(np.array(group[\"Purchase\"]))\n",
    "min_list = []\n",
    "max_list = []\n",
    "mean_list = []\n",
    "for index, row in train_df.iterrows():\n",
    "    name = row[\"User_ID\"]\n",
    "    min_list.append(min_dict.get(name,0))\n",
    "    max_list.append(max_dict.get(name,0))\n",
    "    mean_list.append(mean_dict.get(name,0))\n",
    "train_df[\"User_ID_Min_Purchase\"] = min_list\n",
    "train_df[\"User_ID_Max_Purchase\"] = max_list\n",
    "train_df[\"User_ID_Mean_Purchase\"] = mean_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we look at the correlation values again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#correlation of all other columns with Purchase\n",
    "train_df[train_df.columns[1:]].corr()['Purchase'][:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "High correlation between min,max and mean purchase.\n",
    "<br> I will now do the same for 25, 50 and 75 %ile values.\n",
    "<br> Write a method to calculate all the stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPurchaseStats(target_df,compute_df, feature_name):\n",
    "    feature_grouped = compute_df.groupby(feature_name)\n",
    "    min_dict = {}\n",
    "    max_dict = {}\n",
    "    mean_dict = {}\n",
    "    twentyfive_dict = {}\n",
    "    fifty_dict = {}\n",
    "    seventyfive_dict = {}\n",
    "    for name, group in feature_grouped:\n",
    "        min_dict[name] = min(np.array(group[\"Purchase\"]))\n",
    "        max_dict[name] = max(np.array(group[\"Purchase\"]))\n",
    "        mean_dict[name] = np.mean(np.array(group[\"Purchase\"]))\n",
    "        twentyfive_dict[name] = np.percentile(np.array(group[\"Purchase\"]),25)\n",
    "        fifty_dict[name] = np.percentile(np.array(group[\"Purchase\"]),50)\n",
    "        seventyfive_dict[name] = np.percentile(np.array(group[\"Purchase\"]),75)\n",
    "    min_list = []\n",
    "    max_list = []\n",
    "    mean_list = []\n",
    "    twentyfive_list = []\n",
    "    fifty_list = []\n",
    "    seventyfive_list = []\n",
    "    for index, row in target_df.iterrows():\n",
    "        name = row[feature_name]\n",
    "        min_list.append(min_dict.get(name,0))\n",
    "        max_list.append(max_dict.get(name,0))\n",
    "        mean_list.append(mean_dict.get(name,0))\n",
    "        twentyfive_list.append( twentyfive_dict.get(name,0))\n",
    "        fifty_list.append( fifty_dict.get(name,0))\n",
    "        seventyfive_list.append( seventyfive_dict.get(name,0))\n",
    "    return min_list, max_list, mean_list, twentyfive_list, fifty_list, seventyfive_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "User ID and Purchase stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_price_list, max_price_list, mean_price_list, twentyfive_price_list,fifty_price_list, seventyfive_price_list = getPurchaseStats(train_df,train_df, \"User_ID\")\n",
    "train_df[\"User_ID_Min_Purchase\"] = min_price_list\n",
    "train_df[\"User_ID_Max_Purchase\"] = max_price_list\n",
    "train_df[\"User_ID_Mean_Purchase\"] = mean_price_list\n",
    "train_df[\"User_ID_25Per_Purchase\"] = twentyfive_price_list\n",
    "train_df[\"User_ID_50Per_Purchase\"] = fifty_price_list\n",
    "train_df[\"User_ID_75Per_Purchase\"] = seventyfive_price_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Product_ID and Purchase Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_price_list, max_price_list, mean_price_list, twentyfive_price_list,fifty_price_list, seventyfive_price_list = getPurchaseStats(train_df,train_df, \"Product_ID\")\n",
    "train_df[\"Product_ID_Min_Purchase\"] = min_price_list\n",
    "train_df[\"Product_ID_Max_Purchase\"] = max_price_list\n",
    "train_df[\"Product_ID_Mean_Purchase\"] = mean_price_list\n",
    "train_df[\"Product_ID_25Per_Purchase\"] = twentyfive_price_list\n",
    "train_df[\"Product_ID_50Per_Purchase\"] = fifty_price_list\n",
    "train_df[\"Product_ID_75Per_Purchase\"] = seventyfive_price_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gender and Purchase Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_price_list, max_price_list, mean_price_list, twentyfive_price_list,fifty_price_list, seventyfive_price_list = getPurchaseStats(train_df, train_df, \"Gender\")\n",
    "train_df[\"Gender_Min_Purchase\"] = min_price_list\n",
    "train_df[\"Gender_Max_Purchase\"] = max_price_list\n",
    "train_df[\"Gender_Mean_Purchase\"] = mean_price_list\n",
    "train_df[\"Gender_25Per_Purchase\"] = twentyfive_price_list\n",
    "train_df[\"Gender_50Per_Purchase\"] = fifty_price_list\n",
    "train_df[\"Gender_75Per_Purchase\"] = seventyfive_price_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Age and Purchase stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_price_list, max_price_list, mean_price_list, twentyfive_price_list,fifty_price_list, seventyfive_price_list = getPurchaseStats(train_df,train_df, \"Age\")\n",
    "train_df[\"Age_Min_Purchase\"] = min_price_list\n",
    "train_df[\"Age_Max_Purchase\"] = max_price_list\n",
    "train_df[\"Age_Mean_Purchase\"] = mean_price_list\n",
    "train_df[\"Age_25Per_Purchase\"] = twentyfive_price_list\n",
    "train_df[\"Age_50Per_Purchase\"] = fifty_price_list\n",
    "train_df[\"Age_75Per_Purchase\"] = seventyfive_price_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Occupation and Purchase stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_price_list, max_price_list, mean_price_list, twentyfive_price_list,fifty_price_list, seventyfive_price_list = getPurchaseStats(train_df,train_df, \"Occupation\")\n",
    "train_df[\"Occupation_Min_Purchase\"] = min_price_list\n",
    "train_df[\"Occupation_Max_Purchase\"] = max_price_list\n",
    "train_df[\"Occupation_Mean_Purchase\"] = mean_price_list\n",
    "train_df[\"Occupation_25Per_Purchase\"] = twentyfive_price_list\n",
    "train_df[\"Occupation_50Per_Purchase\"] = fifty_price_list\n",
    "train_df[\"Occupation_75Per_Purchase\"] = seventyfive_price_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "City and Purchase stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_price_list, max_price_list, mean_price_list, twentyfive_price_list,fifty_price_list, seventyfive_price_list = getPurchaseStats(train_df,train_df, \"City_Category\")\n",
    "train_df[\"City_Min_Purchase\"] = min_price_list\n",
    "train_df[\"City_Max_Purchase\"] = max_price_list\n",
    "train_df[\"City_Mean_Purchase\"] = mean_price_list\n",
    "train_df[\"City_25Per_Purchase\"] = twentyfive_price_list\n",
    "train_df[\"City_50Per_Purchase\"] = fifty_price_list\n",
    "train_df[\"City_75Per_Purchase\"] = seventyfive_price_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "stay in current city and Purchase stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_price_list, max_price_list, mean_price_list, twentyfive_price_list,fifty_price_list, seventyfive_price_list = getPurchaseStats(train_df,train_df, \"Stay_In_Current_City_Years\")\n",
    "train_df[\"Stay_Min_Purchase\"] = min_price_list\n",
    "train_df[\"Stay_Max_Purchase\"] = max_price_list\n",
    "train_df[\"Stay_Mean_Purchase\"] = mean_price_list\n",
    "train_df[\"Stay_25Per_Purchase\"] = twentyfive_price_list\n",
    "train_df[\"Stay_50Per_Purchase\"] = fifty_price_list\n",
    "train_df[\"Stay_75Per_Purchase\"] = seventyfive_price_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Marital status and Purchase Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_price_list, max_price_list, mean_price_list, twentyfive_price_list,fifty_price_list, seventyfive_price_list = getPurchaseStats(train_df,train_df, \"Marital_Status\")\n",
    "train_df[\"Marital_Min_Purchase\"] = min_price_list\n",
    "train_df[\"Marital_Max_Purchase\"] = max_price_list\n",
    "train_df[\"Marital_Mean_Purchase\"] = mean_price_list\n",
    "train_df[\"Marital_25Per_Purchase\"] = twentyfive_price_list\n",
    "train_df[\"Marital_50Per_Purchase\"] = fifty_price_list\n",
    "train_df[\"Marital_75Per_Purchase\"] = seventyfive_price_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PC1 and Purchase Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_price_list, max_price_list, mean_price_list, twentyfive_price_list,fifty_price_list, seventyfive_price_list = getPurchaseStats(train_df,train_df, \"Product_Category_1\")\n",
    "train_df[\"PC1_Min_Purchase\"] = min_price_list\n",
    "train_df[\"PC1_Max_Purchase\"] = max_price_list\n",
    "train_df[\"PC1_Mean_Purchase\"] = mean_price_list\n",
    "train_df[\"PC1_25Per_Purchase\"] = twentyfive_price_list\n",
    "train_df[\"PC1_50Per_Purchase\"] = fifty_price_list\n",
    "train_df[\"PC1_75Per_Purchase\"] = seventyfive_price_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PC2 and Purchase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_price_list, max_price_list, mean_price_list, twentyfive_price_list,fifty_price_list, seventyfive_price_list = getPurchaseStats(train_df,train_df, \"Product_Category_2\")\n",
    "train_df[\"PC2_Min_Purchase\"] = min_price_list\n",
    "train_df[\"PC2_Max_Purchase\"] = max_price_list\n",
    "train_df[\"PC2_Mean_Purchase\"] = mean_price_list\n",
    "train_df[\"PC2_25Per_Purchase\"] = twentyfive_price_list\n",
    "train_df[\"PC2_50Per_Purchase\"] = fifty_price_list\n",
    "train_df[\"PC2_75Per_Purchase\"] = seventyfive_price_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PC3 and Purchase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_price_list, max_price_list, mean_price_list, twentyfive_price_list,fifty_price_list, seventyfive_price_list = getPurchaseStats(train_df,train_df, \"Product_Category_3\")\n",
    "train_df[\"PC3_Min_Purchase\"] = min_price_list\n",
    "train_df[\"PC3_Max_Purchase\"] = max_price_list\n",
    "train_df[\"PC3_Mean_Purchase\"] = mean_price_list\n",
    "train_df[\"PC3_25Per_Purchase\"] = twentyfive_price_list\n",
    "train_df[\"PC3_50Per_Purchase\"] = fifty_price_list\n",
    "train_df[\"PC3_75Per_Purchase\"] = seventyfive_price_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, I store the data in a file so that I can use it later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_df.to_csv(r'train_full_feature.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(r'train_full_feature.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before discarding any feature, lets run the model once and see how it performs.So, we split the training data into test and training set\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define X\n",
    "X = train_df.drop(columns=['User_ID','Product_ID','Purchase'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define y\n",
    "y = train_df[\"Purchase\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split training data into training set and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use random forest regressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "RF = RandomForestRegressor(max_depth=5, min_samples_leaf=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit the model\n",
    "RF.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict on X test\n",
    "y_test_pred = RF.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the mean squared error\n",
    "from sklearn import metrics\n",
    "import math\n",
    "mse = metrics.mean_squared_error(y_test, y_test_pred)\n",
    "rmse = math.sqrt(mse)\n",
    "print(rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Method 1 for finding useful features: Using Correlation between the features and the target value\n",
    "<br> Can we improve it more by using features that are more relevalt? Let's find out by finding out the correlation of features with the target column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#finding the correlation of all features against the target\n",
    "corr = train_df[train_df.columns[1:]].corr()['Purchase'][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr.to_csv(r'corr_purchase.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will consider all the features that have a correlation of more than 0.75,0.1, 0.07,0.05,0.04,0.03 and see the results.\n",
    "<br> I found the following observations:\n",
    "<br> corr>0.75: 2551.514009612337\n",
    "<br> corr>0.1: 2567.0259719491337\n",
    "<br> corr>0.08: 2547.717685223172\n",
    "<br> corr>0.07: 2547.3391061981\n",
    "<br> corr>0.05: 2538.0098909567223\n",
    "<br> corr>0.04: 2545.5502808804126\n",
    "<br> corr>0.03: 2542.985008014369"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "import math\n",
    "\n",
    "corr_values = [0.75, 0.1, 0.08, 0.07, 0.05, 0.04, 0.03]\n",
    "\n",
    "for x in corr_values:\n",
    "    feature_list = corr[corr>0.05]\n",
    "    features = feature_list.index.tolist()\n",
    "    features.remove(\"Purchase\")\n",
    "    X = train_df[features]\n",
    "    y = train_df[\"Purchase\"]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "    RF.fit(X_train,y_train)\n",
    "    y_test_pred = RF.predict(X_test)\n",
    "    mse = metrics.mean_squared_error(y_test, y_test_pred)\n",
    "    rmse = math.sqrt(mse)\n",
    "    print(\"corr>\"+str(x)+\": \"+str(rmse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the results above, we can go with features with corr>0.05. Now we will do some hyperparameter tuning of our RandomForestRegressor.\n",
    "<br> \n",
    "<br> Method 2 for finding useful features: Correlation between all features. We remove the ones which have high correlation among themselves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find correlation between all the features\n",
    "corr = train_df.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr.to_csv(r'corr_all_features.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#have a look at the correlation map\n",
    "corr.style.background_gradient(cmap='coolwarm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#have a look at the heatmap for correlation\n",
    "sns.heatmap(corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = train_df.drop(columns=['Product_ID'])\n",
    "columns = np.full((corr.shape[0],), True, dtype=bool)\n",
    "for i in range(corr.shape[0]):\n",
    "    for j in range(i+1, corr.shape[0]):\n",
    "        if corr.iloc[i,j] >= 0.1:\n",
    "            if columns[j]:\n",
    "                columns[j] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_columns = data.columns[columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[selected_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define X\n",
    "X = data.drop(columns=['User_ID'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define y\n",
    "y = train_df[\"Purchase\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split training data into training set and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use random forest regressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "RF = RandomForestRegressor(max_depth=5, min_samples_leaf=100,random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit the model\n",
    "RF.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict on X test\n",
    "y_test_pred = RF.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the mean squared error\n",
    "from sklearn import metrics\n",
    "import math\n",
    "mse = metrics.mean_squared_error(y_test, y_test_pred)\n",
    "rmse = math.sqrt(mse)\n",
    "print(rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Method 3: Using the feature selection in Random Forest regressor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define X\n",
    "X = train_df.drop(columns=['User_ID','Product_ID','Purchase'],axis=1)\n",
    "\n",
    "#define y\n",
    "y = train_df[\"Purchase\"]\n",
    "\n",
    "#split training data into training set and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use random forest regressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "RF = RandomForestRegressor(max_depth=5, min_samples_leaf=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit the model\n",
    "RF.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's look at the features and their scores\n",
    "print (\"Features sorted by their score:\")\n",
    "print (sorted(zip(map(lambda x: round(x, 4), RF.feature_importances_), X_train), reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define X after selecting the top few features\n",
    "X = train_df[[\"Product_ID_Mean_Purchase\",\"User_ID_75Per_Purchase\",\"User_ID_Mean_Purchase\",\"User_ID_25Per_Purchase\",\"User_ID_Max_Purchase\",\"Product_ID_75Per_Purchase\",\"User_ID_50Per_Purchase\",\"Product_ID_50Per_Purchase\",\"Product_ID_25Per_Purchase\",\"User_ID_Min_Purchase\",\"User_ID_Count\",\"Stay_Min_Purchase\",\"Stay_Mean_Purchase\",\"Stay_Max_Purchase\",\"Stay_In_Current_City_Years\",\"Stay_Count\",\"Stay_75Per_Purchase\",\"Stay_50Per_Purchase\",\"Stay_25Per_Purchase\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use random forest regressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "RF = RandomForestRegressor(max_depth=5, min_samples_leaf=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit the model\n",
    "RF.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict on X test\n",
    "y_test_pred = RF.predict(X_test)\n",
    "\n",
    "#get the mean squared error\n",
    "from sklearn import metrics\n",
    "import math\n",
    "mse = metrics.mean_squared_error(y_test, y_test_pred)\n",
    "rmse = math.sqrt(mse)\n",
    "print(rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have done 3 iterations with a few more features and the rmse seems to be stuck at one value.So, we will not do any further iterations.\n",
    "<br> Method 4: Now we will try the same with xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define X\n",
    "X = train_df.drop(columns=['User_ID','Product_ID','Purchase'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define y\n",
    "y = train_df[\"Purchase\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split training data into training set and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor\n",
    "xgb = XGBRegressor(n_estimators=300, max_depth = 10, learning_rate = 0.05, objective = \"reg:squarederror\", min_child_weight  = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = xgb.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the mean squared error\n",
    "from sklearn import metrics\n",
    "import math\n",
    "mse = metrics.mean_squared_error(y_test, y_test_pred)\n",
    "rmse = math.sqrt(mse)\n",
    "print(rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(xgb.feature_importances_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"Features sorted by their score:\")\n",
    "print (sorted(zip(map(lambda x: round(x, 4), xgb.feature_importances_), X_train), reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we will take features that have importance more than 0.0005\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "selection = SelectFromModel(xgb, threshold=0.001, prefit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define X_train after selecting the top few features\n",
    "X_train_selection = selection.transform(X_train)\n",
    "#define X_test after selecting the top few features\n",
    "X_test_selection = selection.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_selected_features = XGBRegressor(n_estimators=1000, learning_rate=0.05)\n",
    "# train model\n",
    "xgb_selected_features.fit(X_train_selection, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict with the new features\n",
    "y_test_pred = xgb_selected_features.predict(X_test_selection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the mean squared error\n",
    "from sklearn import metrics\n",
    "import math\n",
    "mse = metrics.mean_squared_error(y_test, y_test_pred)\n",
    "rmse = math.sqrt(mse)\n",
    "print(rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we know that we can use this model for predicting the purchase for the test data\n",
    "<br> So, we will do all the steps that we did for preprocessing the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the data\n",
    "test_df = pd.read_csv(r'test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df[\"Gender\"] = test_df[\"Gender\"].apply(lambda x: gender_dict[x])\n",
    "test_df[\"Age\"] = test_df[\"Age\"].apply(lambda x: age_dict[x])\n",
    "test_df[\"City_Category\"] = test_df[\"City_Category\"].apply(lambda x: city_dict[x])\n",
    "test_df[\"Stay_In_Current_City_Years\"] = test_df[\"Stay_In_Current_City_Years\"].apply(lambda x: stay_dict[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df[\"User_ID_Count\"] = getCountofVar(test_df,\"User_ID\")\n",
    "test_df[\"Product_ID_Count\"] = getCountofVar(test_df,\"Product_ID\")\n",
    "test_df[\"Gender_Count\"] = getCountofVar(test_df,\"Gender\")\n",
    "test_df[\"Age_Count\"] = getCountofVar(test_df,\"Age\")\n",
    "test_df[\"Occupation_Count\"] = getCountofVar(test_df,\"Occupation\")\n",
    "test_df[\"City_Count\"] = getCountofVar(test_df,\"City_Category\")\n",
    "test_df[\"Stay_Count\"] = getCountofVar(test_df,\"Stay_In_Current_City_Years\")\n",
    "test_df[\"Marital_Status_Count\"] = getCountofVar(test_df,\"Marital_Status\")\n",
    "test_df[\"PC1_Count\"] = getCountofVar(test_df,\"Product_Category_1\")\n",
    "test_df[\"PC2_Count\"] = getCountofVar(test_df,\"Product_Category_2\")\n",
    "test_df[\"PC3_Count\"] = getCountofVar(test_df,\"Product_Category_3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_price_list, max_price_list, mean_price_list, twentyfive_price_list,fifty_price_list, seventyfive_price_list = getPurchaseStats(test_df,train_df, \"User_ID\")\n",
    "test_df[\"User_ID_Min_Purchase\"] = min_price_list\n",
    "test_df[\"User_ID_Max_Purchase\"] = max_price_list\n",
    "test_df[\"User_ID_Mean_Purchase\"] = mean_price_list\n",
    "test_df[\"User_ID_25Per_Purchase\"] = twentyfive_price_list\n",
    "test_df[\"User_ID_50Per_Purchase\"] = fifty_price_list\n",
    "test_df[\"User_ID_75Per_Purchase\"] = seventyfive_price_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_price_list, max_price_list, mean_price_list, twentyfive_price_list,fifty_price_list, seventyfive_price_list = getPurchaseStats(test_df,train_df, \"Product_ID\")\n",
    "test_df[\"Product_ID_Min_Purchase\"] = min_price_list\n",
    "test_df[\"Product_ID_Max_Purchase\"] = max_price_list\n",
    "test_df[\"Product_ID_Mean_Purchase\"] = mean_price_list\n",
    "test_df[\"Product_ID_25Per_Purchase\"] = twentyfive_price_list\n",
    "test_df[\"Product_ID_50Per_Purchase\"] = fifty_price_list\n",
    "test_df[\"Product_ID_75Per_Purchase\"] = seventyfive_price_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_price_list, max_price_list, mean_price_list, twentyfive_price_list,fifty_price_list, seventyfive_price_list = getPurchaseStats(test_df, train_df, \"Gender\")\n",
    "test_df[\"Gender_Min_Purchase\"] = min_price_list\n",
    "test_df[\"Gender_Max_Purchase\"] = max_price_list\n",
    "test_df[\"Gender_Mean_Purchase\"] = mean_price_list\n",
    "test_df[\"Gender_25Per_Purchase\"] = twentyfive_price_list\n",
    "test_df[\"Gender_50Per_Purchase\"] = fifty_price_list\n",
    "test_df[\"Gender_75Per_Purchase\"] = seventyfive_price_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_price_list, max_price_list, mean_price_list, twentyfive_price_list,fifty_price_list, seventyfive_price_list = getPurchaseStats(test_df,train_df, \"Age\")\n",
    "test_df[\"Age_Min_Purchase\"] = min_price_list\n",
    "test_df[\"Age_Max_Purchase\"] = max_price_list\n",
    "test_df[\"Age_Mean_Purchase\"] = mean_price_list\n",
    "test_df[\"Age_25Per_Purchase\"] = twentyfive_price_list\n",
    "test_df[\"Age_50Per_Purchase\"] = fifty_price_list\n",
    "test_df[\"Age_75Per_Purchase\"] = seventyfive_price_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_price_list, max_price_list, mean_price_list, twentyfive_price_list,fifty_price_list, seventyfive_price_list = getPurchaseStats(test_df,train_df, \"Occupation\")\n",
    "test_df[\"Occupation_Min_Purchase\"] = min_price_list\n",
    "test_df[\"Occupation_Max_Purchase\"] = max_price_list\n",
    "test_df[\"Occupation_Mean_Purchase\"] = mean_price_list\n",
    "test_df[\"Occupation_25Per_Purchase\"] = twentyfive_price_list\n",
    "test_df[\"Occupation_50Per_Purchase\"] = fifty_price_list\n",
    "test_df[\"Occupation_75Per_Purchase\"] = seventyfive_price_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_price_list, max_price_list, mean_price_list, twentyfive_price_list,fifty_price_list, seventyfive_price_list = getPurchaseStats(test_df,train_df, \"City_Category\")\n",
    "test_df[\"City_Min_Purchase\"] = min_price_list\n",
    "test_df[\"City_Max_Purchase\"] = max_price_list\n",
    "test_df[\"City_Mean_Purchase\"] = mean_price_list\n",
    "test_df[\"City_25Per_Purchase\"] = twentyfive_price_list\n",
    "test_df[\"City_50Per_Purchase\"] = fifty_price_list\n",
    "test_df[\"City_75Per_Purchase\"] = seventyfive_price_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_price_list, max_price_list, mean_price_list, twentyfive_price_list,fifty_price_list, seventyfive_price_list = getPurchaseStats(test_df,train_df, \"Stay_In_Current_City_Years\")\n",
    "test_df[\"Stay_Min_Purchase\"] = min_price_list\n",
    "test_df[\"Stay_Max_Purchase\"] = max_price_list\n",
    "test_df[\"Stay_Mean_Purchase\"] = mean_price_list\n",
    "test_df[\"Stay_25Per_Purchase\"] = twentyfive_price_list\n",
    "test_df[\"Stay_50Per_Purchase\"] = fifty_price_list\n",
    "test_df[\"Stay_75Per_Purchase\"] = seventyfive_price_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_price_list, max_price_list, mean_price_list, twentyfive_price_list,fifty_price_list, seventyfive_price_list = getPurchaseStats(test_df,train_df, \"Marital_Status\")\n",
    "test_df[\"Marital_Min_Purchase\"] = min_price_list\n",
    "test_df[\"Marital_Max_Purchase\"] = max_price_list\n",
    "test_df[\"Marital_Mean_Purchase\"] = mean_price_list\n",
    "test_df[\"Marital_25Per_Purchase\"] = twentyfive_price_list\n",
    "test_df[\"Marital_50Per_Purchase\"] = fifty_price_list\n",
    "test_df[\"Marital_75Per_Purchase\"] = seventyfive_price_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_price_list, max_price_list, mean_price_list, twentyfive_price_list,fifty_price_list, seventyfive_price_list = getPurchaseStats(test_df,train_df, \"Product_Category_1\")\n",
    "test_df[\"PC1_Min_Purchase\"] = min_price_list\n",
    "test_df[\"PC1_Max_Purchase\"] = max_price_list\n",
    "test_df[\"PC1_Mean_Purchase\"] = mean_price_list\n",
    "test_df[\"PC1_25Per_Purchase\"] = twentyfive_price_list\n",
    "test_df[\"PC1_50Per_Purchase\"] = fifty_price_list\n",
    "test_df[\"PC1_75Per_Purchase\"] = seventyfive_price_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_price_list, max_price_list, mean_price_list, twentyfive_price_list,fifty_price_list, seventyfive_price_list = getPurchaseStats(test_df,train_df, \"Product_Category_2\")\n",
    "test_df[\"PC2_Min_Purchase\"] = min_price_list\n",
    "test_df[\"PC2_Max_Purchase\"] = max_price_list\n",
    "test_df[\"PC2_Mean_Purchase\"] = mean_price_list\n",
    "test_df[\"PC2_25Per_Purchase\"] = twentyfive_price_list\n",
    "test_df[\"PC2_50Per_Purchase\"] = fifty_price_list\n",
    "test_df[\"PC2_75Per_Purchase\"] = seventyfive_price_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_price_list, max_price_list, mean_price_list, twentyfive_price_list,fifty_price_list, seventyfive_price_list = getPurchaseStats(test_df,train_df, \"Product_Category_3\")\n",
    "test_df[\"PC3_Min_Purchase\"] = min_price_list\n",
    "test_df[\"PC3_Max_Purchase\"] = max_price_list\n",
    "test_df[\"PC3_Mean_Purchase\"] = mean_price_list\n",
    "test_df[\"PC3_25Per_Purchase\"] = twentyfive_price_list\n",
    "test_df[\"PC3_50Per_Purchase\"] = fifty_price_list\n",
    "test_df[\"PC3_75Per_Purchase\"] = seventyfive_price_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.to_csv(r'test_full_feature.csv',index=False)\n",
    "#test_df = pd.read_csv(r'test_full_feature.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define test data\n",
    "test_data = test_df.drop(columns=['User_ID','Product_ID'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_selection = selection.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict the test data\n",
    "test_df[\"Purchase\"] = xgb_selected_features.predict(test_data_selection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IDcol = ['User_ID','Product_ID']\n",
    "IDcol.append(\"Purchase\")\n",
    "submission = pd.DataFrame({ x: test_df[x] for x in IDcol})\n",
    "submission.to_csv(r\"submission_xgb1.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
