{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Necessary imports for viewing data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading test and train data set\n",
    "train_df = pd.read_csv(r'E:\\Data hackatons\\AV black friday sales\\train.csv')\n",
    "test_df = pd.read_csv(r'E:\\Data hackatons\\AV black friday sales\\test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's have a look at how the data looks using the head\n",
    "train_df.head()\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 12 columns:\n",
    "User_ID, Product_ID, Gender, Age, Occupation, City_Category, Stay_In_Current_City_Years, Marital_Status, Product_Category_1, Product_Category_2, Product_Category_3, Purchase\n",
    "\n",
    "Target column is Purchase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data***********\n",
      "User_ID                       0.0\n",
      "Product_ID                    0.0\n",
      "Gender                        0.0\n",
      "Age                           0.0\n",
      "Occupation                    0.0\n",
      "City_Category                 0.0\n",
      "Stay_In_Current_City_Years    0.0\n",
      "Marital_Status                0.0\n",
      "Product_Category_1            0.0\n",
      "Product_Category_2            0.0\n",
      "Product_Category_3            0.0\n",
      "Purchase                      0.0\n",
      "dtype: float64\n",
      "Test Data***********\n",
      "User_ID               0.0\n",
      "Product_ID            0.0\n",
      "Gender                0.0\n",
      "Age                   0.0\n",
      "Occupation            0.0\n",
      "                     ... \n",
      "PC3_Max_Purchase      0.0\n",
      "PC3_Mean_Purchase     0.0\n",
      "PC3_25Per_Purchase    0.0\n",
      "PC3_50Per_Purchase    0.0\n",
      "PC3_75Per_Purchase    0.0\n",
      "Length: 88, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#Check for columns with null values\n",
    "print(\"Train Data***********\")\n",
    "print(train_df.isnull().mean() * 100)\n",
    "print(\"Test Data***********\")\n",
    "print(test_df.isnull().mean() * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that Product_Category_2 and Product_Category_3 have null values\n",
    "train_df has 31% and 70% null values in Product_Category_2 and Product_Category_3 respectively\n",
    "test_df has 31% and 70% null values in Product_Category_2 and Product_Category_3 respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#All the columns are categorical. Let's check unique number of entries in each column for Training set\n",
    "print(\"User_ID: \"+str(train_df[\"User_ID\"].unique().shape[0]))\n",
    "print(\"Product_ID: \"+str(train_df[\"Product_ID\"].unique().shape[0]))\n",
    "print(\"Gender: \"+str(train_df[\"Gender\"].unique().shape[0]))\n",
    "print(\"Age: \"+str(train_df[\"Age\"].unique().shape[0]))\n",
    "print(\"Occupation: \"+str(train_df[\"Occupation\"].unique().shape[0]))\n",
    "print(\"City_Category: \"+str(train_df[\"City_Category\"].unique().shape[0]))\n",
    "print(\"Stay_In_Current_City_Years: \"+str(train_df[\"Stay_In_Current_City_Years\"].unique().shape[0]))\n",
    "print(\"Marital_Status: \"+str(train_df[\"Marital_Status\"].unique().shape[0]))\n",
    "print(\"Product_Category_1: \"+str(train_df[\"Product_Category_1\"].unique().shape[0]))\n",
    "print(\"Product_Category_2: \"+str(train_df[\"Product_Category_2\"].unique().shape[0]))\n",
    "print(\"Product_Category_3: \"+str(train_df[\"Product_Category_3\"].unique().shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's check unique number of entries in each column for Test set\n",
    "print(\"User_ID: \"+str(test_df[\"User_ID\"].unique().shape[0]))\n",
    "print(\"Product_ID: \"+str(test_df[\"Product_ID\"].unique().shape[0]))\n",
    "print(\"Gender: \"+str(test_df[\"Gender\"].unique().shape[0]))\n",
    "print(\"Age: \"+str(test_df[\"Age\"].unique().shape[0]))\n",
    "print(\"Occupation: \"+str(test_df[\"Occupation\"].unique().shape[0]))\n",
    "print(\"City_Category: \"+str(test_df[\"City_Category\"].unique().shape[0]))\n",
    "print(\"Stay_In_Current_City_Years: \"+str(test_df[\"Stay_In_Current_City_Years\"].unique().shape[0]))\n",
    "print(\"Marital_Status: \"+str(test_df[\"Marital_Status\"].unique().shape[0]))\n",
    "print(\"Product_Category_1: \"+str(test_df[\"Product_Category_1\"].unique().shape[0]))\n",
    "print(\"Product_Category_2: \"+str(test_df[\"Product_Category_2\"].unique().shape[0]))\n",
    "print(\"Product_Category_3: \"+str(test_df[\"Product_Category_3\"].unique().shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test set has 140 less product ids and 2 less product categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_df['Product_Category_1'].unique())\n",
    "print(test_df['Product_Category_1'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that 19 and 20 are not there in Product_Category_1 for test data. I want to check if the 140 extra product ids in the training data are from the 2 product categories (19 and 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missed_values = train_df.loc[~train_df['Product_ID'].isin(test_df[\"Product_ID\"].unique())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missed_values[\"Product_Category_1\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "so, the 140 extra product ids are not from the product categories 19 and 20.\n",
    "Lets look at the frequency of occurence of user id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['User_ID'].value_counts().describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['User_ID'].value_counts().head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['User_ID'].value_counts(ascending = True).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets see if the value counts have any effect on the purchase. \n",
    "Add the user_id_count as a column to the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_id_grouped = train_df.groupby(\"User_ID\")\n",
    "count_dict = {}\n",
    "for name, group in user_id_grouped:\n",
    "    count_dict[name] = group.shape[0]\n",
    "count_list = []\n",
    "for index, row in train_df.iterrows():\n",
    "    name = row[\"User_ID\"]\n",
    "    count_list.append(count_dict.get(name, 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the correlation of this new field with the purchase "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[\"User_ID_Count\"] = count_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['User_ID_Count'].corr(train_df['Purchase'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The user id count is correlated to the Purchase.Let's look at the count values of other variables, and for that we need to convert the categorical variables into numbers. They are:\n",
    "<br> Gender\n",
    "<br> Age\n",
    "<br> City_Category\n",
    "<br> Stay_In_Current_City\n",
    "<br> Define dictionaries to convert the categorical features into numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_dict = {'F':0, 'M':1}\n",
    "age_dict = {'0-17':0, '18-25':1, '26-35':2, '36-45':3, '46-50':4, '51-55':5, '55+':6}\n",
    "city_dict = {'A':0, 'B':1, 'C':2}\n",
    "stay_dict = {'0':0, '1':1, '2':2, '3':3, '4+':4}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the dictionary above to convert the categorical variables into numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-2c952471557f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Gender\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Gender\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mgender_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Age\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Age\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mage_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"City_Category\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"City_Category\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mcity_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Stay_In_Current_City_Years\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Stay_In_Current_City_Years\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstay_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Anaconda\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self, func, convert_dtype, args, **kwds)\u001b[0m\n\u001b[0;32m   3846\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3847\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3848\u001b[1;33m                 \u001b[0mmapped\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3849\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3850\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m<ipython-input-28-2c952471557f>\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Gender\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Gender\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mgender_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Age\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Age\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mage_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"City_Category\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"City_Category\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mcity_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Stay_In_Current_City_Years\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Stay_In_Current_City_Years\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstay_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 0"
     ]
    }
   ],
   "source": [
    "train_df[\"Gender\"] = train_df[\"Gender\"].apply(lambda x: gender_dict[x])\n",
    "train_df[\"Age\"] = train_df[\"Age\"].apply(lambda x: age_dict[x])\n",
    "train_df[\"City_Category\"] = train_df[\"City_Category\"].apply(lambda x: city_dict[x])\n",
    "train_df[\"Stay_In_Current_City_Years\"] = train_df[\"Stay_In_Current_City_Years\"].apply(lambda x: stay_dict[x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have  alook at the variables now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create a function to give the count of the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCountofVar(dataset_df, var_name):\n",
    "    var_name_grouped = dataset_df.groupby(var_name)\n",
    "    count_dict = {}\n",
    "    for name, group in var_name_grouped:\n",
    "        count_dict[name] = group.shape[0]\n",
    "    count_list = []\n",
    "    for index, row in dataset_df.iterrows():\n",
    "        name = row[var_name]\n",
    "        count_list.append(count_dict.get(name, 0))\n",
    "    return count_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get counts for all other features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[\"Product_ID_Count\"] = getCountofVar(train_df,\"Product_ID\")\n",
    "train_df[\"Gender_Count\"] = getCountofVar(train_df,\"Gender\")\n",
    "train_df[\"Age_Count\"] = getCountofVar(train_df,\"Age\")\n",
    "train_df[\"Occupation_Count\"] = getCountofVar(train_df,\"Occupation\")\n",
    "train_df[\"City_Count\"] = getCountofVar(train_df,\"City_Category\")\n",
    "train_df[\"Stay_Count\"] = getCountofVar(train_df,\"Stay_In_Current_City_Years\")\n",
    "train_df[\"Marital_Status_Count\"] = getCountofVar(train_df,\"Marital_Status\")\n",
    "train_df[\"PC1_Count\"] = getCountofVar(train_df,\"Product_Category_1\")\n",
    "train_df[\"PC2_Count\"] = getCountofVar(train_df,\"Product_Category_2\")\n",
    "train_df[\"PC3_Count\"] = getCountofVar(train_df,\"Product_Category_3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Look at the columns once\n",
    "train_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets look at the correlation of the features\n",
    "corr = train_df.corr()\n",
    "corr.style.background_gradient(cmap='coolwarm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#correlation of all other columns with Purchase\n",
    "train_df[train_df.columns[1:]].corr()['Purchase'][:]\n",
    "#to see the correlation in a csv file\n",
    "#corr.to_csv(r'C:\\Users\\User\\Desktop\\corr_table.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I want to check if the minimum values, max values, mean of the features have any effect on the purchase.\n",
    "<br> But, before this I will impute the missing values with 0. I did not impute the missing values earlier as it will cause the counts to reflect values. Right now the count for missing values is 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For every user, we find the min purchase, max purchase and mean purchase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_user_id_grouped = train_df.groupby(\"User_ID\")\n",
    "min_dict = {}\n",
    "max_dict = {}\n",
    "mean_dict = {}\n",
    "for name, group in train_df_user_id_grouped:\n",
    "    min_dict[name] = min(np.array(group[\"Purchase\"]))\n",
    "    max_dict[name] = max(np.array(group[\"Purchase\"]))\n",
    "    mean_dict[name] = np.mean(np.array(group[\"Purchase\"]))\n",
    "min_list = []\n",
    "max_list = []\n",
    "mean_list = []\n",
    "for index, row in train_df.iterrows():\n",
    "    name = row[\"User_ID\"]\n",
    "    min_list.append(min_dict.get(name,0))\n",
    "    max_list.append(max_dict.get(name,0))\n",
    "    mean_list.append(mean_dict.get(name,0))\n",
    "train_df[\"User_ID_Min_Purchase\"] = min_list\n",
    "train_df[\"User_ID_Max_Purchase\"] = max_list\n",
    "train_df[\"User_ID_Mean_Purchase\"] = mean_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we look at the correlation values again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#correlation of all other columns with Purchase\n",
    "train_df[train_df.columns[1:]].corr()['Purchase'][:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "High correlation between min,max and mean purchase.\n",
    "<br> I will now do the same for 25, 50 and 75 %ile values.\n",
    "<br> Write a method to calculate all the stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPurchaseStats(target_df,compute_df, feature_name):\n",
    "    feature_grouped = compute_df.groupby(feature_name)\n",
    "    min_dict = {}\n",
    "    max_dict = {}\n",
    "    mean_dict = {}\n",
    "    twentyfive_dict = {}\n",
    "    fifty_dict = {}\n",
    "    seventyfive_dict = {}\n",
    "    for name, group in feature_grouped:\n",
    "        min_dict[name] = min(np.array(group[\"Purchase\"]))\n",
    "        max_dict[name] = max(np.array(group[\"Purchase\"]))\n",
    "        mean_dict[name] = np.mean(np.array(group[\"Purchase\"]))\n",
    "        twentyfive_dict[name] = np.percentile(np.array(group[\"Purchase\"]),25)\n",
    "        fifty_dict[name] = np.percentile(np.array(group[\"Purchase\"]),50)\n",
    "        seventyfive_dict[name] = np.percentile(np.array(group[\"Purchase\"]),75)\n",
    "    min_list = []\n",
    "    max_list = []\n",
    "    mean_list = []\n",
    "    twentyfive_list = []\n",
    "    fifty_list = []\n",
    "    seventyfive_list = []\n",
    "    for index, row in target_df.iterrows():\n",
    "        name = row[feature_name]\n",
    "        min_list.append(min_dict.get(name,0))\n",
    "        max_list.append(max_dict.get(name,0))\n",
    "        mean_list.append(mean_dict.get(name,0))\n",
    "        twentyfive_list.append( twentyfive_dict.get(name,0))\n",
    "        fifty_list.append( fifty_dict.get(name,0))\n",
    "        seventyfive_list.append( seventyfive_dict.get(name,0))\n",
    "    return min_list, max_list, mean_list, twentyfive_list, fifty_list, seventyfive_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "User ID and Purchase stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_price_list, max_price_list, mean_price_list, twentyfive_price_list,fifty_price_list, seventyfive_price_list = getPurchaseStats(train_df,train_df, \"User_ID\")\n",
    "train_df[\"User_ID_Min_Purchase\"] = min_price_list\n",
    "train_df[\"User_ID_Max_Purchase\"] = max_price_list\n",
    "train_df[\"User_ID_Mean_Purchase\"] = mean_price_list\n",
    "train_df[\"User_ID_25Per_Purchase\"] = twentyfive_price_list\n",
    "train_df[\"User_ID_50Per_Purchase\"] = fifty_price_list\n",
    "train_df[\"User_ID_75Per_Purchase\"] = seventyfive_price_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Product_ID and Purchase Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_price_list, max_price_list, mean_price_list, twentyfive_price_list,fifty_price_list, seventyfive_price_list = getPurchaseStats(train_df,train_df, \"Product_ID\")\n",
    "train_df[\"Product_ID_Min_Purchase\"] = min_price_list\n",
    "train_df[\"Product_ID_Max_Purchase\"] = max_price_list\n",
    "train_df[\"Product_ID_Mean_Purchase\"] = mean_price_list\n",
    "train_df[\"Product_ID_25Per_Purchase\"] = twentyfive_price_list\n",
    "train_df[\"Product_ID_50Per_Purchase\"] = fifty_price_list\n",
    "train_df[\"Product_ID_75Per_Purchase\"] = seventyfive_price_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gender and Purchase Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_price_list, max_price_list, mean_price_list, twentyfive_price_list,fifty_price_list, seventyfive_price_list = getPurchaseStats(train_df, train_df, \"Gender\")\n",
    "train_df[\"Gender_Min_Purchase\"] = min_price_list\n",
    "train_df[\"Gender_Max_Purchase\"] = max_price_list\n",
    "train_df[\"Gender_Mean_Purchase\"] = mean_price_list\n",
    "train_df[\"Gender_25Per_Purchase\"] = twentyfive_price_list\n",
    "train_df[\"Gender_50Per_Purchase\"] = fifty_price_list\n",
    "train_df[\"Gender_75Per_Purchase\"] = seventyfive_price_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Age and Purchase stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_price_list, max_price_list, mean_price_list, twentyfive_price_list,fifty_price_list, seventyfive_price_list = getPurchaseStats(train_df,train_df, \"Age\")\n",
    "train_df[\"Age_Min_Purchase\"] = min_price_list\n",
    "train_df[\"Age_Max_Purchase\"] = max_price_list\n",
    "train_df[\"Age_Mean_Purchase\"] = mean_price_list\n",
    "train_df[\"Age_25Per_Purchase\"] = twentyfive_price_list\n",
    "train_df[\"Age_50Per_Purchase\"] = fifty_price_list\n",
    "train_df[\"Age_75Per_Purchase\"] = seventyfive_price_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Occupation and Purchase stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_price_list, max_price_list, mean_price_list, twentyfive_price_list,fifty_price_list, seventyfive_price_list = getPurchaseStats(train_df,train_df, \"Occupation\")\n",
    "train_df[\"Occupation_Min_Purchase\"] = min_price_list\n",
    "train_df[\"Occupation_Max_Purchase\"] = max_price_list\n",
    "train_df[\"Occupation_Mean_Purchase\"] = mean_price_list\n",
    "train_df[\"Occupation_25Per_Purchase\"] = twentyfive_price_list\n",
    "train_df[\"Occupation_50Per_Purchase\"] = fifty_price_list\n",
    "train_df[\"Occupation_75Per_Purchase\"] = seventyfive_price_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "City and Purchase stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_price_list, max_price_list, mean_price_list, twentyfive_price_list,fifty_price_list, seventyfive_price_list = getPurchaseStats(train_df,train_df, \"City_Category\")\n",
    "train_df[\"City_Min_Purchase\"] = min_price_list\n",
    "train_df[\"City_Max_Purchase\"] = max_price_list\n",
    "train_df[\"City_Mean_Purchase\"] = mean_price_list\n",
    "train_df[\"City_25Per_Purchase\"] = twentyfive_price_list\n",
    "train_df[\"City_50Per_Purchase\"] = fifty_price_list\n",
    "train_df[\"City_75Per_Purchase\"] = seventyfive_price_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "stay in current city and Purchase stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_price_list, max_price_list, mean_price_list, twentyfive_price_list,fifty_price_list, seventyfive_price_list = getPurchaseStats(train_df,train_df, \"Stay_In_Current_City_Years\")\n",
    "train_df[\"Stay_Min_Purchase\"] = min_price_list\n",
    "train_df[\"Stay_Max_Purchase\"] = max_price_list\n",
    "train_df[\"Stay_Mean_Purchase\"] = mean_price_list\n",
    "train_df[\"Stay_25Per_Purchase\"] = twentyfive_price_list\n",
    "train_df[\"Stay_50Per_Purchase\"] = fifty_price_list\n",
    "train_df[\"Stay_75Per_Purchase\"] = seventyfive_price_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Marital status and Purchase Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_price_list, max_price_list, mean_price_list, twentyfive_price_list,fifty_price_list, seventyfive_price_list = getPurchaseStats(train_df,train_df, \"Marital_Status\")\n",
    "train_df[\"Marital_Min_Purchase\"] = min_price_list\n",
    "train_df[\"Marital_Max_Purchase\"] = max_price_list\n",
    "train_df[\"Marital_Mean_Purchase\"] = mean_price_list\n",
    "train_df[\"Marital_25Per_Purchase\"] = twentyfive_price_list\n",
    "train_df[\"Marital_50Per_Purchase\"] = fifty_price_list\n",
    "train_df[\"Marital_75Per_Purchase\"] = seventyfive_price_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PC1 and Purchase Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_price_list, max_price_list, mean_price_list, twentyfive_price_list,fifty_price_list, seventyfive_price_list = getPurchaseStats(train_df,train_df, \"Product_Category_1\")\n",
    "train_df[\"PC1_Min_Purchase\"] = min_price_list\n",
    "train_df[\"PC1_Max_Purchase\"] = max_price_list\n",
    "train_df[\"PC1_Mean_Purchase\"] = mean_price_list\n",
    "train_df[\"PC1_25Per_Purchase\"] = twentyfive_price_list\n",
    "train_df[\"PC1_50Per_Purchase\"] = fifty_price_list\n",
    "train_df[\"PC1_75Per_Purchase\"] = seventyfive_price_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PC2 and Purchase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_price_list, max_price_list, mean_price_list, twentyfive_price_list,fifty_price_list, seventyfive_price_list = getPurchaseStats(train_df,train_df, \"Product_Category_2\")\n",
    "train_df[\"PC2_Min_Purchase\"] = min_price_list\n",
    "train_df[\"PC2_Max_Purchase\"] = max_price_list\n",
    "train_df[\"PC2_Mean_Purchase\"] = mean_price_list\n",
    "train_df[\"PC2_25Per_Purchase\"] = twentyfive_price_list\n",
    "train_df[\"PC2_50Per_Purchase\"] = fifty_price_list\n",
    "train_df[\"PC2_75Per_Purchase\"] = seventyfive_price_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PC3 and Purchase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_price_list, max_price_list, mean_price_list, twentyfive_price_list,fifty_price_list, seventyfive_price_list = getPurchaseStats(train_df,train_df, \"Product_Category_3\")\n",
    "train_df[\"PC3_Min_Purchase\"] = min_price_list\n",
    "train_df[\"PC3_Max_Purchase\"] = max_price_list\n",
    "train_df[\"PC3_Mean_Purchase\"] = mean_price_list\n",
    "train_df[\"PC3_25Per_Purchase\"] = twentyfive_price_list\n",
    "train_df[\"PC3_50Per_Purchase\"] = fifty_price_list\n",
    "train_df[\"PC3_75Per_Purchase\"] = seventyfive_price_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, I store the data in a file so that I can use it later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_df.to_csv(r'E:\\Data hackatons\\AV black friday sales\\train_full_feature.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(r'E:\\Data hackatons\\AV black friday sales\\train_full_feature.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before discarding any feature, lets run the model once and see how it performs.So, we split the training data into test and training set\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define X\n",
    "X = train_df.drop(columns=['User_ID','Product_ID','Purchase'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define y\n",
    "y = train_df[\"Purchase\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split training data into training set and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use random forest regressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "RF = RandomForestRegressor(max_depth=5, min_samples_leaf=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit the model\n",
    "RF.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict on X test\n",
    "y_test_pred = RF.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the mean squared error\n",
    "from sklearn import metrics\n",
    "import math\n",
    "mse = metrics.mean_squared_error(y_test, y_test_pred)\n",
    "rmse = math.sqrt(mse)\n",
    "print(rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Method 1 for finding useful features: Using Correlation between the features and the target value\n",
    "<br> Can we improve it more by using features that are more relevalt? Let's find out by finding out the correlation of features with the target column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#finding the correlation of all features against the target\n",
    "corr = train_df[train_df.columns[1:]].corr()['Purchase'][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr.to_csv(r'E:\\Data hackatons\\AV black friday sales\\corr_purchase.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will consider all the features that have a correlation of more than 0.75,0.1, 0.07,0.05,0.04,0.03 and see the results.\n",
    "<br> I found the following observations:\n",
    "<br> corr>0.75: 2551.514009612337\n",
    "<br> corr>0.1: 2567.0259719491337\n",
    "<br> corr>0.08: 2547.717685223172\n",
    "<br> corr>0.07: 2547.3391061981\n",
    "<br> corr>0.05: 2538.0098909567223\n",
    "<br> corr>0.04: 2545.5502808804126\n",
    "<br> corr>0.03: 2542.985008014369"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "import math\n",
    "\n",
    "corr_values = [0.75, 0.1, 0.08, 0.07, 0.05, 0.04, 0.03]\n",
    "\n",
    "for x in corr_values:\n",
    "    feature_list = corr[corr>0.05]\n",
    "    features = feature_list.index.tolist()\n",
    "    features.remove(\"Purchase\")\n",
    "    X = train_df[features]\n",
    "    y = train_df[\"Purchase\"]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "    RF.fit(X_train,y_train)\n",
    "    y_test_pred = RF.predict(X_test)\n",
    "    mse = metrics.mean_squared_error(y_test, y_test_pred)\n",
    "    rmse = math.sqrt(mse)\n",
    "    print(\"corr>\"+str(x)+\": \"+str(rmse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the results above, we can go with features with corr>0.05. Now we will do some hyperparameter tuning of our RandomForestRegressor.\n",
    "<br> I will do Randomized Search CV to find the best parameters later in the notebook\n",
    "<br> \n",
    "<br> Method 2 for finding useful features: Correlation between all features. We remove the ones which have high correlation among themselves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find correlation between all the features\n",
    "corr = train_df.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr.to_csv(r'E:\\Data hackatons\\AV black friday sales\\corr_all_features.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#have a look at the correlation map\n",
    "corr.style.background_gradient(cmap='coolwarm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#have a look at the heatmap for correlation\n",
    "sns.heatmap(corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = train_df.drop(columns=['Product_ID'])\n",
    "columns = np.full((corr.shape[0],), True, dtype=bool)\n",
    "for i in range(corr.shape[0]):\n",
    "    for j in range(i+1, corr.shape[0]):\n",
    "        if corr.iloc[i,j] >= 0.1:\n",
    "            if columns[j]:\n",
    "                columns[j] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_columns = data.columns[columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[selected_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define X\n",
    "X = data.drop(columns=['User_ID'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define y\n",
    "y = train_df[\"Purchase\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split training data into training set and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use random forest regressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "RF = RandomForestRegressor(max_depth=5, min_samples_leaf=100,random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit the model\n",
    "RF.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict on X test\n",
    "y_test_pred = RF.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the mean squared error\n",
    "from sklearn import metrics\n",
    "import math\n",
    "mse = metrics.mean_squared_error(y_test, y_test_pred)\n",
    "rmse = math.sqrt(mse)\n",
    "print(rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Method 3: Using the feature selection in Random Forest regressor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define X\n",
    "X = train_df.drop(columns=['User_ID','Product_ID','Purchase'],axis=1)\n",
    "\n",
    "#define y\n",
    "y = train_df[\"Purchase\"]\n",
    "\n",
    "#split training data into training set and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use random forest regressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "RF = RandomForestRegressor(max_depth=5, min_samples_leaf=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit the model\n",
    "RF.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's look at the features and their scores\n",
    "print (\"Features sorted by their score:\")\n",
    "print (sorted(zip(map(lambda x: round(x, 4), RF.feature_importances_), X_train), reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define X after selecting the top few features\n",
    "X = train_df[[\"Product_ID_Mean_Purchase\",\"User_ID_75Per_Purchase\",\"User_ID_Mean_Purchase\",\"User_ID_25Per_Purchase\",\"User_ID_Max_Purchase\",\"Product_ID_75Per_Purchase\",\"User_ID_50Per_Purchase\",\"Product_ID_50Per_Purchase\",\"Product_ID_25Per_Purchase\",\"User_ID_Min_Purchase\",\"User_ID_Count\",\"Stay_Min_Purchase\",\"Stay_Mean_Purchase\",\"Stay_Max_Purchase\",\"Stay_In_Current_City_Years\",\"Stay_Count\",\"Stay_75Per_Purchase\",\"Stay_50Per_Purchase\",\"Stay_25Per_Purchase\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use random forest regressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "RF = RandomForestRegressor(max_depth=5, min_samples_leaf=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit the model\n",
    "RF.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict on X test\n",
    "y_test_pred = RF.predict(X_test)\n",
    "\n",
    "#get the mean squared error\n",
    "from sklearn import metrics\n",
    "import math\n",
    "mse = metrics.mean_squared_error(y_test, y_test_pred)\n",
    "rmse = math.sqrt(mse)\n",
    "print(rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have done 3 iterations with a few more features and the rmse seems to be stuck at one value.So, we will not do any further iterations.\n",
    "<br> Method 4: Now we will try the same with xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define X\n",
    "X = train_df.drop(columns=['User_ID','Product_ID','Purchase'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define y\n",
    "y = train_df[\"Purchase\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split training data into training set and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(440054, 86)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(110014, 86)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor\n",
    "xgb = XGBRegressor(n_estimators=1000, learning_rate=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19:38:08] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "             colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
       "             importance_type='gain', learning_rate=0.05, max_delta_step=0,\n",
       "             max_depth=3, min_child_weight=1, missing=None, n_estimators=1000,\n",
       "             n_jobs=1, nthread=None, objective='reg:linear', random_state=0,\n",
       "             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "             silent=None, subsample=1, verbosity=1)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = xgb.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2444.9273196038525\n"
     ]
    }
   ],
   "source": [
    "#get the mean squared error\n",
    "from sklearn import metrics\n",
    "import math\n",
    "mse = metrics.mean_squared_error(y_test, y_test_pred)\n",
    "rmse = math.sqrt(mse)\n",
    "print(rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.9876465e-03 1.3517028e-03 6.6246116e-04 8.3649537e-04 5.6555693e-04\n",
      " 3.4135481e-04 1.4658089e-03 5.6706718e-04 6.0337031e-04 1.9646459e-03\n",
      " 9.4769296e-04 0.0000000e+00 8.8327209e-04 8.1652292e-04 3.7492742e-04\n",
      " 7.6457596e-04 0.0000000e+00 5.7051715e-04 6.8848504e-04 5.8336748e-04\n",
      " 7.0602726e-04 2.4166501e-03 8.6763585e-03 7.5712698e-03 3.8120893e-03\n",
      " 9.2351176e-03 1.3045226e-03 2.8741399e-03 9.0283388e-01 4.4284058e-03\n",
      " 4.1518575e-03 1.2750125e-02 0.0000000e+00 0.0000000e+00 0.0000000e+00\n",
      " 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00\n",
      " 7.5191120e-04 0.0000000e+00 0.0000000e+00 1.3141887e-03 0.0000000e+00\n",
      " 8.4565335e-04 8.0333988e-04 9.3002291e-04 7.8399033e-05 6.8166334e-04\n",
      " 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00\n",
      " 0.0000000e+00 0.0000000e+00 0.0000000e+00 5.7822664e-04 3.1911474e-04\n",
      " 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00\n",
      " 0.0000000e+00 0.0000000e+00 0.0000000e+00 4.4141281e-03 0.0000000e+00\n",
      " 1.3322341e-03 0.0000000e+00 1.9688422e-03 0.0000000e+00 7.4683077e-04\n",
      " 1.1582375e-03 7.9378340e-04 4.8399245e-04 7.0362183e-04 8.5252640e-04\n",
      " 7.0114358e-04 5.9829070e-04 5.8793684e-04 6.7506824e-04 5.3792825e-04\n",
      " 4.0701401e-04]\n"
     ]
    }
   ],
   "source": [
    "print(xgb.feature_importances_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features sorted by their score:\n",
      "[(0.9028, 'Product_ID_Mean_Purchase'), (0.0128, 'Product_ID_75Per_Purchase'), (0.0092, 'User_ID_75Per_Purchase'), (0.0087, 'User_ID_Mean_Purchase'), (0.0076, 'User_ID_25Per_Purchase'), (0.0044, 'Product_ID_25Per_Purchase'), (0.0044, 'PC1_Min_Purchase'), (0.0042, 'Product_ID_50Per_Purchase'), (0.0038, 'User_ID_50Per_Purchase'), (0.003, 'Gender'), (0.0029, 'Product_ID_Max_Purchase'), (0.0024, 'User_ID_Max_Purchase'), (0.002, 'User_ID_Count'), (0.002, 'PC1_50Per_Purchase'), (0.0015, 'Product_Category_1'), (0.0014, 'Age'), (0.0013, 'Product_ID_Min_Purchase'), (0.0013, 'PC1_Mean_Purchase'), (0.0013, 'Age_75Per_Purchase'), (0.0012, 'PC2_Max_Purchase'), (0.0009, 'Product_ID_Count'), (0.0009, 'PC2_75Per_Purchase'), (0.0009, 'Occupation_25Per_Purchase'), (0.0009, 'Age_Count'), (0.0008, 'Stay_Count'), (0.0008, 'PC2_Mean_Purchase'), (0.0008, 'Occupation_Mean_Purchase'), (0.0008, 'Occupation_Max_Purchase'), (0.0008, 'Occupation_Count'), (0.0008, 'City_Category'), (0.0008, 'Age_Mean_Purchase'), (0.0007, 'User_ID_Min_Purchase'), (0.0007, 'PC3_Min_Purchase'), (0.0007, 'PC3_25Per_Purchase'), (0.0007, 'PC2_Min_Purchase'), (0.0007, 'PC2_Count'), (0.0007, 'PC2_50Per_Purchase'), (0.0007, 'Occupation_75Per_Purchase'), (0.0007, 'Occupation'), (0.0006, 'Stay_Mean_Purchase'), (0.0006, 'Stay_In_Current_City_Years'), (0.0006, 'Product_Category_3'), (0.0006, 'Product_Category_2'), (0.0006, 'PC3_Mean_Purchase'), (0.0006, 'PC3_Max_Purchase'), (0.0006, 'PC3_Count'), (0.0006, 'PC1_Count'), (0.0005, 'PC3_50Per_Purchase'), (0.0005, 'PC2_25Per_Purchase'), (0.0004, 'PC3_75Per_Purchase'), (0.0004, 'City_Count'), (0.0003, 'Stay_25Per_Purchase'), (0.0003, 'Marital_Status'), (1e-04, 'Occupation_50Per_Purchase'), (0.0, 'Stay_Min_Purchase'), (0.0, 'Stay_Max_Purchase'), (0.0, 'Stay_75Per_Purchase'), (0.0, 'Stay_50Per_Purchase'), (0.0, 'PC1_Max_Purchase'), (0.0, 'PC1_75Per_Purchase'), (0.0, 'PC1_25Per_Purchase'), (0.0, 'Occupation_Min_Purchase'), (0.0, 'Marital_Status_Count'), (0.0, 'Marital_Min_Purchase'), (0.0, 'Marital_Mean_Purchase'), (0.0, 'Marital_Max_Purchase'), (0.0, 'Marital_75Per_Purchase'), (0.0, 'Marital_50Per_Purchase'), (0.0, 'Marital_25Per_Purchase'), (0.0, 'Gender_Min_Purchase'), (0.0, 'Gender_Mean_Purchase'), (0.0, 'Gender_Max_Purchase'), (0.0, 'Gender_Count'), (0.0, 'Gender_75Per_Purchase'), (0.0, 'Gender_50Per_Purchase'), (0.0, 'Gender_25Per_Purchase'), (0.0, 'City_Min_Purchase'), (0.0, 'City_Mean_Purchase'), (0.0, 'City_Max_Purchase'), (0.0, 'City_75Per_Purchase'), (0.0, 'City_50Per_Purchase'), (0.0, 'City_25Per_Purchase'), (0.0, 'Age_Min_Purchase'), (0.0, 'Age_Max_Purchase'), (0.0, 'Age_50Per_Purchase'), (0.0, 'Age_25Per_Purchase')]\n"
     ]
    }
   ],
   "source": [
    "print (\"Features sorted by their score:\")\n",
    "print (sorted(zip(map(lambda x: round(x, 4), xgb.feature_importances_), X_train), reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we will take features that have importance more than 0.0005\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "selection = SelectFromModel(xgb, threshold=0.001, prefit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define X_train after selecting the top few features\n",
    "X_train_selection = selection.transform(X_train)\n",
    "#define X_test after selecting the top few features\n",
    "X_test_selection = selection.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:39:17] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "             colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
       "             importance_type='gain', learning_rate=0.05, max_delta_step=0,\n",
       "             max_depth=3, min_child_weight=1, missing=None, n_estimators=1000,\n",
       "             n_jobs=1, nthread=None, objective='reg:linear', random_state=0,\n",
       "             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "             silent=None, subsample=1, verbosity=1)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_selected_features = XGBRegressor(n_estimators=1000, learning_rate=0.05)\n",
    "# train model\n",
    "xgb_selected_features.fit(X_train_selection, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict with the new features\n",
    "y_test_pred = xgb_selected_features.predict(X_test_selection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2446.865162601363\n"
     ]
    }
   ],
   "source": [
    "#get the mean squared error\n",
    "from sklearn import metrics\n",
    "import math\n",
    "mse = metrics.mean_squared_error(y_test, y_test_pred)\n",
    "rmse = math.sqrt(mse)\n",
    "print(rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we know that we can use this model for predicting the purchase for the test data\n",
    "<br> So, we will do all the steps that we did for preprocessing the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the data\n",
    "test_df = pd.read_csv(r'E:\\Data hackatons\\AV black friday sales\\test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df[\"Gender\"] = test_df[\"Gender\"].apply(lambda x: gender_dict[x])\n",
    "test_df[\"Age\"] = test_df[\"Age\"].apply(lambda x: age_dict[x])\n",
    "test_df[\"City_Category\"] = test_df[\"City_Category\"].apply(lambda x: city_dict[x])\n",
    "test_df[\"Stay_In_Current_City_Years\"] = test_df[\"Stay_In_Current_City_Years\"].apply(lambda x: stay_dict[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df[\"User_ID_Count\"] = getCountofVar(test_df,\"User_ID\")\n",
    "test_df[\"Product_ID_Count\"] = getCountofVar(test_df,\"Product_ID\")\n",
    "test_df[\"Gender_Count\"] = getCountofVar(test_df,\"Gender\")\n",
    "test_df[\"Age_Count\"] = getCountofVar(test_df,\"Age\")\n",
    "test_df[\"Occupation_Count\"] = getCountofVar(test_df,\"Occupation\")\n",
    "test_df[\"City_Count\"] = getCountofVar(test_df,\"City_Category\")\n",
    "test_df[\"Stay_Count\"] = getCountofVar(test_df,\"Stay_In_Current_City_Years\")\n",
    "test_df[\"Marital_Status_Count\"] = getCountofVar(test_df,\"Marital_Status\")\n",
    "test_df[\"PC1_Count\"] = getCountofVar(test_df,\"Product_Category_1\")\n",
    "test_df[\"PC2_Count\"] = getCountofVar(test_df,\"Product_Category_2\")\n",
    "test_df[\"PC3_Count\"] = getCountofVar(test_df,\"Product_Category_3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_price_list, max_price_list, mean_price_list, twentyfive_price_list,fifty_price_list, seventyfive_price_list = getPurchaseStats(test_df,train_df, \"User_ID\")\n",
    "test_df[\"User_ID_Min_Purchase\"] = min_price_list\n",
    "test_df[\"User_ID_Max_Purchase\"] = max_price_list\n",
    "test_df[\"User_ID_Mean_Purchase\"] = mean_price_list\n",
    "test_df[\"User_ID_25Per_Purchase\"] = twentyfive_price_list\n",
    "test_df[\"User_ID_50Per_Purchase\"] = fifty_price_list\n",
    "test_df[\"User_ID_75Per_Purchase\"] = seventyfive_price_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_price_list, max_price_list, mean_price_list, twentyfive_price_list,fifty_price_list, seventyfive_price_list = getPurchaseStats(test_df,train_df, \"Product_ID\")\n",
    "test_df[\"Product_ID_Min_Purchase\"] = min_price_list\n",
    "test_df[\"Product_ID_Max_Purchase\"] = max_price_list\n",
    "test_df[\"Product_ID_Mean_Purchase\"] = mean_price_list\n",
    "test_df[\"Product_ID_25Per_Purchase\"] = twentyfive_price_list\n",
    "test_df[\"Product_ID_50Per_Purchase\"] = fifty_price_list\n",
    "test_df[\"Product_ID_75Per_Purchase\"] = seventyfive_price_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_price_list, max_price_list, mean_price_list, twentyfive_price_list,fifty_price_list, seventyfive_price_list = getPurchaseStats(test_df, train_df, \"Gender\")\n",
    "test_df[\"Gender_Min_Purchase\"] = min_price_list\n",
    "test_df[\"Gender_Max_Purchase\"] = max_price_list\n",
    "test_df[\"Gender_Mean_Purchase\"] = mean_price_list\n",
    "test_df[\"Gender_25Per_Purchase\"] = twentyfive_price_list\n",
    "test_df[\"Gender_50Per_Purchase\"] = fifty_price_list\n",
    "test_df[\"Gender_75Per_Purchase\"] = seventyfive_price_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_price_list, max_price_list, mean_price_list, twentyfive_price_list,fifty_price_list, seventyfive_price_list = getPurchaseStats(test_df,train_df, \"Age\")\n",
    "test_df[\"Age_Min_Purchase\"] = min_price_list\n",
    "test_df[\"Age_Max_Purchase\"] = max_price_list\n",
    "test_df[\"Age_Mean_Purchase\"] = mean_price_list\n",
    "test_df[\"Age_25Per_Purchase\"] = twentyfive_price_list\n",
    "test_df[\"Age_50Per_Purchase\"] = fifty_price_list\n",
    "test_df[\"Age_75Per_Purchase\"] = seventyfive_price_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_price_list, max_price_list, mean_price_list, twentyfive_price_list,fifty_price_list, seventyfive_price_list = getPurchaseStats(test_df,train_df, \"Occupation\")\n",
    "test_df[\"Occupation_Min_Purchase\"] = min_price_list\n",
    "test_df[\"Occupation_Max_Purchase\"] = max_price_list\n",
    "test_df[\"Occupation_Mean_Purchase\"] = mean_price_list\n",
    "test_df[\"Occupation_25Per_Purchase\"] = twentyfive_price_list\n",
    "test_df[\"Occupation_50Per_Purchase\"] = fifty_price_list\n",
    "test_df[\"Occupation_75Per_Purchase\"] = seventyfive_price_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_price_list, max_price_list, mean_price_list, twentyfive_price_list,fifty_price_list, seventyfive_price_list = getPurchaseStats(test_df,train_df, \"City_Category\")\n",
    "test_df[\"City_Min_Purchase\"] = min_price_list\n",
    "test_df[\"City_Max_Purchase\"] = max_price_list\n",
    "test_df[\"City_Mean_Purchase\"] = mean_price_list\n",
    "test_df[\"City_25Per_Purchase\"] = twentyfive_price_list\n",
    "test_df[\"City_50Per_Purchase\"] = fifty_price_list\n",
    "test_df[\"City_75Per_Purchase\"] = seventyfive_price_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_price_list, max_price_list, mean_price_list, twentyfive_price_list,fifty_price_list, seventyfive_price_list = getPurchaseStats(test_df,train_df, \"Stay_In_Current_City_Years\")\n",
    "test_df[\"Stay_Min_Purchase\"] = min_price_list\n",
    "test_df[\"Stay_Max_Purchase\"] = max_price_list\n",
    "test_df[\"Stay_Mean_Purchase\"] = mean_price_list\n",
    "test_df[\"Stay_25Per_Purchase\"] = twentyfive_price_list\n",
    "test_df[\"Stay_50Per_Purchase\"] = fifty_price_list\n",
    "test_df[\"Stay_75Per_Purchase\"] = seventyfive_price_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_price_list, max_price_list, mean_price_list, twentyfive_price_list,fifty_price_list, seventyfive_price_list = getPurchaseStats(test_df,train_df, \"Marital_Status\")\n",
    "test_df[\"Marital_Min_Purchase\"] = min_price_list\n",
    "test_df[\"Marital_Max_Purchase\"] = max_price_list\n",
    "test_df[\"Marital_Mean_Purchase\"] = mean_price_list\n",
    "test_df[\"Marital_25Per_Purchase\"] = twentyfive_price_list\n",
    "test_df[\"Marital_50Per_Purchase\"] = fifty_price_list\n",
    "test_df[\"Marital_75Per_Purchase\"] = seventyfive_price_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_price_list, max_price_list, mean_price_list, twentyfive_price_list,fifty_price_list, seventyfive_price_list = getPurchaseStats(test_df,train_df, \"Product_Category_1\")\n",
    "test_df[\"PC1_Min_Purchase\"] = min_price_list\n",
    "test_df[\"PC1_Max_Purchase\"] = max_price_list\n",
    "test_df[\"PC1_Mean_Purchase\"] = mean_price_list\n",
    "test_df[\"PC1_25Per_Purchase\"] = twentyfive_price_list\n",
    "test_df[\"PC1_50Per_Purchase\"] = fifty_price_list\n",
    "test_df[\"PC1_75Per_Purchase\"] = seventyfive_price_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_price_list, max_price_list, mean_price_list, twentyfive_price_list,fifty_price_list, seventyfive_price_list = getPurchaseStats(test_df,train_df, \"Product_Category_2\")\n",
    "test_df[\"PC2_Min_Purchase\"] = min_price_list\n",
    "test_df[\"PC2_Max_Purchase\"] = max_price_list\n",
    "test_df[\"PC2_Mean_Purchase\"] = mean_price_list\n",
    "test_df[\"PC2_25Per_Purchase\"] = twentyfive_price_list\n",
    "test_df[\"PC2_50Per_Purchase\"] = fifty_price_list\n",
    "test_df[\"PC2_75Per_Purchase\"] = seventyfive_price_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_price_list, max_price_list, mean_price_list, twentyfive_price_list,fifty_price_list, seventyfive_price_list = getPurchaseStats(test_df,train_df, \"Product_Category_3\")\n",
    "test_df[\"PC3_Min_Purchase\"] = min_price_list\n",
    "test_df[\"PC3_Max_Purchase\"] = max_price_list\n",
    "test_df[\"PC3_Mean_Purchase\"] = mean_price_list\n",
    "test_df[\"PC3_25Per_Purchase\"] = twentyfive_price_list\n",
    "test_df[\"PC3_50Per_Purchase\"] = fifty_price_list\n",
    "test_df[\"PC3_75Per_Purchase\"] = seventyfive_price_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.to_csv(r'E:\\Data hackatons\\AV black friday sales\\test_full_feature.csv',index=False)\n",
    "#test_df = pd.read_csv(r'E:\\Data hackatons\\AV black friday sales\\test_full_feature.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define test data\n",
    "test_data = test_df.drop(columns=['User_ID','Product_ID'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_selection = selection.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict the test data\n",
    "test_df[\"Purchase\"] = xgb_selected_features.predict(test_data_selection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "IDcol = ['User_ID','Product_ID']\n",
    "IDcol.append(\"Purchase\")\n",
    "submission = pd.DataFrame({ x: test_df[x] for x in IDcol})\n",
    "submission.to_csv(r\"E:\\Data hackatons\\AV black friday sales\\submission_xgb1.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
