1. Necessary imports for viewing data
import pandas as pd
import numpy as np
import seaborn as sns

2. Loading test and train data set
train_df = pd.read_csv(r'E:\Data hackatons\AV black friday sales\train.csv')
test_df = pd.read_csv(r'E:\Data hackatons\AV black friday sales\test.csv')

3. Let's have a look at how the data looks using the head
train_df.head()
test_df.head()

4. There are 12 columns:
User_ID
Product_ID
Gender
Age
Occupation
City_Category
Stay_In_Current_City_Years
Marital_Status
Product_Category_1
Product_Category_2
Product_Category_3
Purchase

Target column is Purchase

5. #Check for columns with null values
print("Train Data***********")
print(train_df.isnull().mean() * 100)
print("Test Data***********")
print(test_df.isnull().mean() * 100)

6. We can see that Product_Category_2 and Product_Category_3 have null values
train_df has 31% and 70% null values in Product_Category_2 and Product_Category_3 respectively
test_df has 31% and 70% null values in Product_Category_2 and Product_Category_3 respectively

7.#All the columns are categorical. Let's check unique number of entries in each column for Training set
print("User_ID: "+str(train_df["User_ID"].unique().shape[0]))
print("Product_ID: "+str(train_df["Product_ID"].unique().shape[0]))
print("Gender: "+str(train_df["Gender"].unique().shape[0]))
print("Age: "+str(train_df["Age"].unique().shape[0]))
print("Occupation: "+str(train_df["Occupation"].unique().shape[0]))
print("City_Category: "+str(train_df["City_Category"].unique().shape[0]))
print("Stay_In_Current_City_Years: "+str(train_df["Stay_In_Current_City_Years"].unique().shape[0]))
print("Marital_Status: "+str(train_df["Marital_Status"].unique().shape[0]))
print("Product_Category_1: "+str(train_df["Product_Category_1"].unique().shape[0]))
print("Product_Category_2: "+str(train_df["Product_Category_2"].unique().shape[0]))
print("Product_Category_3: "+str(train_df["Product_Category_3"].unique().shape[0]))

User_ID:5891
Product_ID:3631
Gender:2
Age:7
Occupation:21
City_Category:3
Stay_In_Current_City_Years:5
Marital_Status:2
Product_Category_1:20
Product_Category_2:18
Product_Category_3:16

8.#Let's check unique number of entries in each column for Test set
print("User_ID: "+str(test_df["User_ID"].unique().shape[0]))
print("Product_ID: "+str(test_df["Product_ID"].unique().shape[0]))
print("Gender: "+str(test_df["Gender"].unique().shape[0]))
print("Age: "+str(test_df["Age"].unique().shape[0]))
print("Occupation: "+str(test_df["Occupation"].unique().shape[0]))
print("City_Category: "+str(test_df["City_Category"].unique().shape[0]))
print("Stay_In_Current_City_Years: "+str(test_df["Stay_In_Current_City_Years"].unique().shape[0]))
print("Marital_Status: "+str(test_df["Marital_Status"].unique().shape[0]))
print("Product_Category_1: "+str(test_df["Product_Category_1"].unique().shape[0]))
print("Product_Category_2: "+str(test_df["Product_Category_2"].unique().shape[0]))
print("Product_Category_3: "+str(test_df["Product_Category_3"].unique().shape[0]))
User_ID: 5891
Product_ID: 3491
Gender: 2
Age: 7
Occupation: 21
City_Category: 3
Stay_In_Current_City_Years: 5
Marital_Status: 2
Product_Category_1: 18
Product_Category_2: 18
Product_Category_3: 16

9. Test set has 140 less product ids and 2 less product categories.
print(train_df['Product_Category_1'].unique())
print(test_df['Product_Category_1'].unique())

10. We can see that 19 and 20 are not there in Product_Category_1 for test data. I want to check if the 140 extra product ids in the training data are from the 2 product categories (19 and 20)
missed_values = train_df.loc[~train_df['Product_ID'].isin(test_df["Product_ID"].unique())]
missed_values["Product_Category_1"].unique()

11. so, the 140 extra product ids are not from the product categories 19 and 20.
Lets look at the frequency of occurence of user id

train_df['User_ID'].value_counts().describe()

count    5891.000000
mean       93.374300
std       107.190049
min         6.000000
25%        26.000000
50%        54.000000
75%       117.000000
max      1026.000000
Name: User_ID, dtype: float64

train_df['User_ID'].value_counts().head(10)
1001680    1026
1004277     979
1001941     898
1001181     862
1000889     823
1003618     767
1001150     752
1001015     740
1005795     729
1005831     727

train_df['User_ID'].value_counts(ascending = True).head(10)
1000708    6
1005608    7
1004991    7
1002111    7
1000094    7
1002690    7
1005391    7
1005810    7
1004192    8
1005904    8
Name: User_ID, dtype: int64

12. Lets see if the value counts have any effect on the purchase. 
Add the user_id_count as a column to the data

user_id_grouped = train_df.groupby("User_ID")
count_dict = {}
for name, group in user_id_grouped:
    count_dict[name] = group.shape[0]
count_list = []
for index, row in train_df.iterrows():
    name = row["User_ID"]
    count_list.append(count_dict.get(name, 0))

13. Let's look at the correlation of this new field with the purchase 
train_df["User_ID_Count"] = count_list
train_df['User_ID_Count'].corr(train_df['Purchase'])

The user id count is correlated to the Purchase.Let's look at the count values of other variables, and for that we need to convert the categorical variables into numbers. They are:
Gender
Age
City_Category
Stay_In_Current_City

14. Define dictionaries to convert the categorical features into numeric
gender_dict = {'F':0, 'M':1}
age_dict = {'0-17':0, '18-25':1, '26-35':2, '36-45':3, '46-50':4, '51-55':5, '55+':6}
city_dict = {'A':0, 'B':1, 'C':2}
stay_dict = {'0':0, '1':1, '2':2, '3':3, '4+':4}

15. Use the dictionary above to convert the categorical variables into numeric
train_df["Gender"] = train_df["Gender"].apply(lambda x: gender_dict[x])
train_df["Age"] = train_df["Age"].apply(lambda x: age_dict[x])
train_df["City_Category"] = train_df["City_Category"].apply(lambda x: city_dict[x])
train_df["Stay_In_Current_City_Years"] = train_df["Stay_In_Current_City_Years"].apply(lambda x: stay_dict[x])

16. Let's have  alook at the variables now
train_df.head()
train_df.describe

17. Now let's create a function to give the count of the features

def getCountofVar(dataset_df, var_name):
    var_name_grouped = dataset_df.groupby(var_name)
    count_dict = {}
    for name, group in var_name_grouped:
        count_dict[name] = group.shape[0]
    count_list = []
    for index, row in dataset_df.iterrows():
        name = row[var_name]
        count_list.append(count_dict.get(name, 0))
    return count_list

18. get counts for all other features
train_df["Product_ID_Count"] = getCountofVar(train_df,"Product_ID")
train_df["Gender_Count"] = getCountofVar(train_df,"Gender")
train_df["Age_Count"] = getCountofVar(train_df,"Age")
train_df["Occupation_Count"] = getCountofVar(train_df,"Occupation")
train_df["City_Count"] = getCountofVar(train_df,"City_Category")
train_df["Stay_Count"] = getCountofVar(train_df,"Stay_In_Current_City_Years")
train_df["Marital_Status_Count"] = getCountofVar(train_df,"Marital_Status")
train_df["PC1_Count"] = getCountofVar(train_df,"Product_Category_1")
train_df["PC2_Count"] = getCountofVar(train_df,"Product_Category_2")
train_df["PC3_Count"] = getCountofVar(train_df,"Product_Category_3")

19. Look at the columns once
train_df.columns

20. #correlation of all other columns with Purchase
train_df[train_df.columns[1:]].corr()['Purchase'][:]
#to see the correlation in a csv file
#corr.to_csv(r'C:\Users\User\Desktop\corr_table.csv', index=False)

Gender                        0.060346
Age                           0.015839
Occupation                    0.020833
City_Category                 0.061914
Stay_In_Current_City_Years    0.005422
Marital_Status               -0.000463
Product_Category_1           -0.343703
Product_Category_2           -0.209918
Product_Category_3           -0.022006
Purchase                      1.000000
User_ID_Count                -0.090356
Product_ID_Count              0.308641
Gender_Count                  0.060346
Age_Count                    -0.002070
Occupation_Count             -0.002757
City_Count                   -0.003439
Stay_Count                   -0.000768
Marital_Status_Count          0.000463
PC1_Count                    -0.033405
PC2_Count                     0.190383
PC3_Count                     0.282797
Name: Purchase, dtype: float64

21. Now I want to check if the minimum values, max values, mean of the features have any effect on the purchase. 
But, before this I will impute the missing values with 0. I did not impute the missing values earlier as it will cause the counts to reflect values. Right now the count for missing values is 0.
train_df.fillna(0, inplace=True)

train_df_user_id_grouped = train_df.groupby("User_ID")
min_dict = {}
max_dict = {}
mean_dict = {}
for name, group in train_df_user_id_grouped:
    min_dict[name] = min(np.array(group["Purchase"]))
    max_dict[name] = max(np.array(group["Purchase"]))
    mean_dict[name] = np.mean(np.array(group["Purchase"]))
min_list = []
max_list = []
mean_list = []
for index, row in train_df.iterrows():
    name = row["User_ID"]
    min_list.append(min_dict.get(name,0))
    max_list.append(max_dict.get(name,0))
    mean_list.append(mean_dict.get(name,0))
train_df["User_ID_Min_Purchase"] = min_list
train_df["User_ID_Max_Purchase"] = max_list
train_df["User_ID_Mean_Purchase"] = mean_list

22. Now we look at the correlation table again and see that there is high correlation of the values with the purchase values. I will now do the same for 25, 50 and 75 %ile values.
Write a method to calculate all the stats
def getPurchaseStats(target_df,compute_df, feature_name):
    feature_grouped = compute_df.groupby(feature_name)
    min_dict = {}
    max_dict = {}
    mean_dict = {}
    twentyfive_dict = {}
    fifty_dict = {}
    seventyfive_dict = {}
    for name, group in feature_grouped:
        min_dict[name] = min(np.array(group["Purchase"]))
        max_dict[name] = max(np.array(group["Purchase"]))
        mean_dict[name] = np.mean(np.array(group["Purchase"]))
        twentyfive_dict[name] = np.percentile(np.array(group["Purchase"]),25)
        fifty_dict[name] = np.percentile(np.array(group["Purchase"]),50)
        seventyfive_dict[name] = np.percentile(np.array(group["Purchase"]),75)
    min_list = []
    max_list = []
    mean_list = []
    twentyfive_list = []
    fifty_list = []
    seventyfive_list = []
    for index, row in target_df.iterrows():
        name = row[feature_name]
        min_list.append(min_dict.get(name,0))
        max_list.append(max_dict.get(name,0))
        mean_list.append(mean_dict.get(name,0))
        twentyfive_list.append( twentyfive_dict.get(name,0))
        fifty_list.append( fifty_dict.get(name,0))
        seventyfive_list.append( seventyfive_dict.get(name,0))
    return min_list, max_list, mean_list, twentyfive_list, fifty_list, seventyfive_list
23. User ID and Purchase stats
min_price_list, max_price_list, mean_price_list, twentyfive_price_list,fifty_price_list, seventyfive_price_list = getPurchaseStats(train_df,train_df, "User_ID")
train_df["User_ID_Min_Purchase"] = min_price_list
train_df["User_ID_Max_Purchase"] = max_price_list
train_df["User_ID_Mean_Purchase"] = mean_price_list
train_df["User_ID_25Per_Purchase"] = twentyfive_price_list
train_df["User_ID_50Per_Purchase"] = fifty_price_list
train_df["User_ID_75Per_Purchase"] = seventyfive_price_list

24. Product_ID and Purchase Stats
min_price_list, max_price_list, mean_price_list, twentyfive_price_list,fifty_price_list, seventyfive_price_list = getPurchaseStats(train_df,train_df, "Product_ID")
train_df["Product_ID_Min_Purchase"] = min_price_list
train_df["Product_ID_Max_Purchase"] = max_price_list
train_df["Product_ID_Mean_Purchase"] = mean_price_list
train_df["Product_ID_25Per_Purchase"] = twentyfive_price_list
train_df["Product_ID_50Per_Purchase"] = fifty_price_list
train_df["Product_ID_75Per_Purchase"] = seventyfive_price_list

25. Gender and Purchase Stats
min_price_list, max_price_list, mean_price_list, twentyfive_price_list,fifty_price_list, seventyfive_price_list = getPurchaseStats(train_df, "Gender")
train_df["Gender_Min_Purchase"] = min_price_list
train_df["Gender_Max_Purchase"] = max_price_list
train_df["Gender_Mean_Purchase"] = mean_price_list
train_df["Gender_25Per_Purchase"] = twentyfive_price_list
train_df["Gender_50Per_Purchase"] = fifty_price_list
train_df["Gender_75Per_Purchase"] = seventyfive_price_list

23. Stats for Age
min_price_list, max_price_list, mean_price_list, twentyfive_price_list,fifty_price_list, seventyfive_price_list = getPurchaseStats(train_df,train_df, "Age")
train_df["Age_Min_Purchase"] = min_price_list
train_df["Age_Max_Purchase"] = max_price_list
train_df["Age_Mean_Purchase"] = mean_price_list
train_df["Age_25Per_Purchase"] = twentyfive_price_list
train_df["Age_50Per_Purchase"] = fifty_price_list
train_df["Age_75Per_Purchase"] = seventyfive_price_list

24. Stats for Occupation
min_price_list, max_price_list, mean_price_list, twentyfive_price_list,fifty_price_list, seventyfive_price_list = getPurchaseStats(train_df, "Occupation")
train_df["Occupation_Min_Purchase"] = min_price_list
train_df["Occupation_Max_Purchase"] = max_price_list
train_df["Occupation_Mean_Purchase"] = mean_price_list
train_df["Occupation_25Per_Purchase"] = twentyfive_price_list
train_df["Occupation_50Per_Purchase"] = fifty_price_list
train_df["Occupation_75Per_Purchase"] = seventyfive_price_list

25. Stats for City
min_price_list, max_price_list, mean_price_list, twentyfive_price_list,fifty_price_list, seventyfive_price_list = getPurchaseStats(train_df, "City_Category")
train_df["City_Min_Purchase"] = min_price_list
train_df["City_Max_Purchase"] = max_price_list
train_df["City_Mean_Purchase"] = mean_price_list
train_df["City_25Per_Purchase"] = twentyfive_price_list
train_df["City_50Per_Purchase"] = fifty_price_list
train_df["City_75Per_Purchase"] = seventyfive_price_list

26. Stats for stay in current city
min_price_list, max_price_list, mean_price_list, twentyfive_price_list,fifty_price_list, seventyfive_price_list = getPurchaseStats(train_df, "Stay_In_Current_City_Years")
train_df["Stay_Min_Purchase"] = min_price_list
train_df["Stay_Max_Purchase"] = max_price_list
train_df["Stay_Mean_Purchase"] = mean_price_list
train_df["Stay_25Per_Purchase"] = twentyfive_price_list
train_df["Stay_50Per_Purchase"] = fifty_price_list
train_df["Stay_75Per_Purchase"] = seventyfive_price_list

27. Stats for Marital status
min_price_list, max_price_list, mean_price_list, twentyfive_price_list,fifty_price_list, seventyfive_price_list = getPurchaseStats(train_df, "Marital_Status")
train_df["Marital_Min_Purchase"] = min_price_list
train_df["Marital_Max_Purchase"] = max_price_list
train_df["Marital_Mean_Purchase"] = mean_price_list
train_df["Marital_25Per_Purchase"] = twentyfive_price_list
train_df["Marital_50Per_Purchase"] = fifty_price_list
train_df["Marital_75Per_Purchase"] = seventyfive_price_list

28. Stats for PC1
min_price_list, max_price_list, mean_price_list, twentyfive_price_list,fifty_price_list, seventyfive_price_list = getPurchaseStats(train_df, "Product_Category_1")
train_df["PC1_Min_Purchase"] = min_price_list
train_df["PC1_Max_Purchase"] = max_price_list
train_df["PC1_Mean_Purchase"] = mean_price_list
train_df["PC1_25Per_Purchase"] = twentyfive_price_list
train_df["PC1_50Per_Purchase"] = fifty_price_list
train_df["PC1_75Per_Purchase"] = seventyfive_price_list

28. Stats for PC2
min_price_list, max_price_list, mean_price_list, twentyfive_price_list,fifty_price_list, seventyfive_price_list = getPurchaseStats(train_df, "Product_Category_2")
train_df["PC2_Min_Purchase"] = min_price_list
train_df["PC2_Max_Purchase"] = max_price_list
train_df["PC2_Mean_Purchase"] = mean_price_list
train_df["PC2_25Per_Purchase"] = twentyfive_price_list
train_df["PC2_50Per_Purchase"] = fifty_price_list
train_df["PC2_75Per_Purchase"] = seventyfive_price_list

29. Stats for PC3
min_price_list, max_price_list, mean_price_list, twentyfive_price_list,fifty_price_list, seventyfive_price_list = getPurchaseStats(train_df, "Product_Category_3")
train_df["PC3_Min_Purchase"] = min_price_list
train_df["PC3_Max_Purchase"] = max_price_list
train_df["PC3_Mean_Purchase"] = mean_price_list
train_df["PC3_25Per_Purchase"] = twentyfive_price_list
train_df["PC3_50Per_Purchase"] = fifty_price_list
train_df["PC3_75Per_Purchase"] = seventyfive_price_list

30. Before discarding any feature, lets run the model once and see how it performs.So, we split the training data into test and training set
from sklearn.model_selection import train_test_split

#define X
X = train_df.drop(columns=['User_ID','Product_ID','Purchase'],axis=1)

#define y
y = train_df["Purchase"]

#split training data into training set and test set
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

#use random forest regressor
from sklearn.ensemble import RandomForestRegressor
RF = RandomForestRegressor(max_depth=5, min_samples_leaf=100,random_state=123)

#fit the model
RF.fit(X_train,y_train)

#predict on X test
y_test_pred = RF.predict(X_test)

#get the mean squared error
from sklearn import metrics
import math
mse = metrics.mean_squared_error(y_test, y_test_pred)
rmse = math.sqrt(mse)
print(rmse)

2567.749800866933

31. Can we improve it more by using features that are more relevalt? Let's find out by finding out the correlation of features with the target column

from sklearn import metrics
import math

corr_values = [0.75, 0.1, 0.08, 0.07, 0.05, 0.04, 0.03]

for x in corr_values:
    feature_list = corr[corr>0.05]
    features = feature_list.index.tolist()
    features.remove("Purchase")
    X = train_df[features]
    y = train_df["Purchase"]
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)
    RF.fit(X_train,y_train)
    y_test_pred = RF.predict(X_test)
    mse = metrics.mean_squared_error(y_test, y_test_pred)
    rmse = math.sqrt(mse)
    print("corr>"+str(x)+": "+str(rmse))

corr>0.75: 2551.514009612337
corr>0.1: 2567.0259719491337
corr>0.08: 2547.717685223172
corr>0.07: 2547.3391061981
corr>0.05: 2538.0098909567223
corr>0.04: 2545.5502808804126
corr>0.03: 2542.985008014369

32. Based on the results above, we can go with features with corr>0.05. 
Method 2 for finding useful features: Correlation between all features. We remove the ones which have high correlation among themselves

#find correlation between all the features
corr = train_df.corr()

#have a look at the correlation map
corr.style.background_gradient(cmap='coolwarm')

#have a look at the heatmap for correlation
sns.heatmap(corr)

data = train_df.drop(columns=['Product_ID'])
columns = np.full((corr.shape[0],), True, dtype=bool)
for i in range(corr.shape[0]):
    for j in range(i+1, corr.shape[0]):
        if corr.iloc[i,j] >= 0.1:
            if columns[j]:
                columns[j] = False
selected_columns = data.columns[columns]
data = data[selected_columns]

#define X
X = data.drop(columns=['User_ID'],axis=1)

#define y
y = train_df["Purchase"]

#split training data into training set and test set
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

#use random forest regressor
from sklearn.ensemble import RandomForestRegressor
RF = RandomForestRegressor(max_depth=5, min_samples_leaf=100,random_state=123)

#fit the model
RF.fit(X_train,y_train)

#predict on X test
y_test_pred = RF.predict(X_test)

#get the mean squared error
from sklearn import metrics
import math
mse = metrics.mean_squared_error(y_test, y_test_pred)
rmse = math.sqrt(mse)
print(rmse)

3260.3120537193813

33. Method 3: Using the feature selection in scikit learn

#define X
X = train_df.drop(columns=['User_ID','Product_ID','Purchase'],axis=1)

#define y
y = train_df["Purchase"]

#split training data into training set and test set
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

#use random forest regressor
from sklearn.ensemble import RandomForestRegressor
RF = RandomForestRegressor(max_depth=5, min_samples_leaf=100)

#fit the model
RF.fit(X_train,y_train)

#Let's look at the features and their scores
print ("Features sorted by their score:")
print (sorted(zip(map(lambda x: round(x, 4), RF.feature_importances_), X_train), reverse=True))

#define X after selecting the top few features
X = train_df[["Product_ID_Mean_Purchase","User_ID_75Per_Purchase","User_ID_Mean_Purchase","User_ID_25Per_Purchase","User_ID_Max_Purchase","Product_ID_75Per_Purchase","User_ID_50Per_Purchase","Product_ID_50Per_Purchase","Product_ID_25Per_Purchase"]]

#use random forest regressor
from sklearn.ensemble import RandomForestRegressor
RF = RandomForestRegressor(max_depth=5, min_samples_leaf=100)

#fit the model
RF.fit(X_train,y_train)

#predict on X test
y_test_pred = RF.predict(X_test)

#get the mean squared error
from sklearn import metrics
import math
mse = metrics.mean_squared_error(y_test, y_test_pred)
rmse = math.sqrt(mse)
print(rmse)

2545.7137892897854

34. I have done 3 iterations with a few more features and the rmse seems to be stuck at one value.So, we will not do any further iterations.
<br> Now we will try the same with xgboost
#define X
X = train_df.drop(columns=['User_ID','Product_ID','Purchase'],axis=1)

#define y
y = train_df["Purchase"]

#split training data into training set and test set
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

from xgboost import XGBRegressor
xgb = XGBRegressor(n_estimators=2000, learning_rate=0.05)

xgb.fit(X_train, y_train)

y_test_pred = xgb.predict(X_test)

#get the mean squared error
from sklearn import metrics
import math
mse = metrics.mean_squared_error(y_test, y_test_pred)
rmse = math.sqrt(mse)
print(rmse)

2444

#Let's look at the features and their scores
print ("Features sorted by their score:")
print (sorted(zip(map(lambda x: round(x, 4), xgb.feature_importances_), X_train), reverse=True))

#we will take features that have importance more than 0
from sklearn.feature_selection import SelectFromModel
selection = SelectFromModel(xgb, threshold=0.001, prefit=True)
#define X_train after selecting the top few features
X_train_selection = selection.transform(X_train)

# train model
xgb.fit(X_train_selection, y_train)

#define X_train after selecting the top few features
X_test_selection = selection.transform(X_test)

#predict with the new features
y_test_pred = xgb.predict(X_test)

#get the mean squared error
from sklearn import metrics
import math
mse = metrics.mean_squared_error(y_test, y_test_pred)
rmse = math.sqrt(mse)
print(rmse)
2433

35. So, now we know that we can use this model for predicting the purchase for the test data
So, we will do all the steps that we did for preprocessing the training data

#Load the data
test_df = pd.read_csv(r'E:\Data hackatons\AV black friday sales\test.csv')

test_df["Gender"] = test_df["Gender"].apply(lambda x: gender_dict[x])
test_df["Age"] = test_df["Age"].apply(lambda x: age_dict[x])
test_df["City_Category"] = test_df["City_Category"].apply(lambda x: city_dict[x])
test_df["Stay_In_Current_City_Years"] = test_df["Stay_In_Current_City_Years"].apply(lambda x: stay_dict[x])


test_df["User_ID_Count"] = getCountofVar(test_df,"User_ID")
test_df["Product_ID_Count"] = getCountofVar(test_df,"Product_ID")
test_df["Gender_Count"] = getCountofVar(test_df,"Gender")
test_df["Age_Count"] = getCountofVar(test_df,"Age")
test_df["Occupation_Count"] = getCountofVar(test_df,"Occupation")
test_df["City_Count"] = getCountofVar(test_df,"City_Category")
test_df["Stay_Count"] = getCountofVar(test_df,"Stay_In_Current_City_Years")
test_df["Marital_Status_Count"] = getCountofVar(test_df,"Marital_Status")
test_df["PC1_Count"] = getCountofVar(test_df,"Product_Category_1")
test_df["PC2_Count"] = getCountofVar(test_df,"Product_Category_2")
test_df["PC3_Count"] = getCountofVar(test_df,"Product_Category_3")

train_df.fillna(0, inplace=True)

min_price_list, max_price_list, mean_price_list, twentyfive_price_list,fifty_price_list, seventyfive_price_list = getPurchaseStats(test_df,train_df, "User_ID")
test_df["User_ID_Min_Purchase"] = min_price_list
test_df["User_ID_Max_Purchase"] = max_price_list
test_df["User_ID_Mean_Purchase"] = mean_price_list
test_df["User_ID_25Per_Purchase"] = twentyfive_price_list
test_df["User_ID_50Per_Purchase"] = fifty_price_list
test_df["User_ID_75Per_Purchase"] = seventyfive_price_list

min_price_list, max_price_list, mean_price_list, twentyfive_price_list,fifty_price_list, seventyfive_price_list = getPurchaseStats(test_df,train_df, "Product_ID")
test_df["Product_ID_Min_Purchase"] = min_price_list
test_df["Product_ID_Max_Purchase"] = max_price_list
test_df["Product_ID_Mean_Purchase"] = mean_price_list
test_df["Product_ID_25Per_Purchase"] = twentyfive_price_list
test_df["Product_ID_50Per_Purchase"] = fifty_price_list
test_df["Product_ID_75Per_Purchase"] = seventyfive_price_list

min_price_list, max_price_list, mean_price_list, twentyfive_price_list,fifty_price_list, seventyfive_price_list = getPurchaseStats(test_df, train_df, "Gender")
test_df["Gender_Min_Purchase"] = min_price_list
test_df["Gender_Max_Purchase"] = max_price_list
test_df["Gender_Mean_Purchase"] = mean_price_list
test_df["Gender_25Per_Purchase"] = twentyfive_price_list
test_df["Gender_50Per_Purchase"] = fifty_price_list
test_df["Gender_75Per_Purchase"] = seventyfive_price_list

min_price_list, max_price_list, mean_price_list, twentyfive_price_list,fifty_price_list, seventyfive_price_list = getPurchaseStats(test_df,train_df, "Age")
test_df["Age_Min_Purchase"] = min_price_list
test_df["Age_Max_Purchase"] = max_price_list
test_df["Age_Mean_Purchase"] = mean_price_list
test_df["Age_25Per_Purchase"] = twentyfive_price_list
test_df["Age_50Per_Purchase"] = fifty_price_list
test_df["Age_75Per_Purchase"] = seventyfive_price_list

min_price_list, max_price_list, mean_price_list, twentyfive_price_list,fifty_price_list, seventyfive_price_list = getPurchaseStats(test_df,train_df, "Occupation")
test_df["Occupation_Min_Purchase"] = min_price_list
test_df["Occupation_Max_Purchase"] = max_price_list
test_df["Occupation_Mean_Purchase"] = mean_price_list
test_df["Occupation_25Per_Purchase"] = twentyfive_price_list
test_df["Occupation_50Per_Purchase"] = fifty_price_list
test_df["Occupation_75Per_Purchase"] = seventyfive_price_list

min_price_list, max_price_list, mean_price_list, twentyfive_price_list,fifty_price_list, seventyfive_price_list = getPurchaseStats(test_df,train_df, "City_Category")
test_df["City_Min_Purchase"] = min_price_list
test_df["City_Max_Purchase"] = max_price_list
test_df["City_Mean_Purchase"] = mean_price_list
test_df["City_25Per_Purchase"] = twentyfive_price_list
test_df["City_50Per_Purchase"] = fifty_price_list
test_df["City_75Per_Purchase"] = seventyfive_price_list

min_price_list, max_price_list, mean_price_list, twentyfive_price_list,fifty_price_list, seventyfive_price_list = getPurchaseStats(test_df,train_df, "Stay_In_Current_City_Years")
test_df["Stay_Min_Purchase"] = min_price_list
test_df["Stay_Max_Purchase"] = max_price_list
test_df["Stay_Mean_Purchase"] = mean_price_list
test_df["Stay_25Per_Purchase"] = twentyfive_price_list
test_df["Stay_50Per_Purchase"] = fifty_price_list
test_df["Stay_75Per_Purchase"] = seventyfive_price_list

min_price_list, max_price_list, mean_price_list, twentyfive_price_list,fifty_price_list, seventyfive_price_list = getPurchaseStats(test_df,train_df, "Marital_Status")
test_df["Marital_Min_Purchase"] = min_price_list
test_df["Marital_Max_Purchase"] = max_price_list
test_df["Marital_Mean_Purchase"] = mean_price_list
test_df["Marital_25Per_Purchase"] = twentyfive_price_list
test_df["Marital_50Per_Purchase"] = fifty_price_list
test_df["Marital_75Per_Purchase"] = seventyfive_price_list

min_price_list, max_price_list, mean_price_list, twentyfive_price_list,fifty_price_list, seventyfive_price_list = getPurchaseStats(test_df,train_df, "Product_Category_1")
test_df["PC1_Min_Purchase"] = min_price_list
test_df["PC1_Max_Purchase"] = max_price_list
test_df["PC1_Mean_Purchase"] = mean_price_list
test_df["PC1_25Per_Purchase"] = twentyfive_price_list
test_df["PC1_50Per_Purchase"] = fifty_price_list
test_df["PC1_75Per_Purchase"] = seventyfive_price_list

min_price_list, max_price_list, mean_price_list, twentyfive_price_list,fifty_price_list, seventyfive_price_list = getPurchaseStats(test_df,train_df, "Product_Category_2")
test_df["PC2_Min_Purchase"] = min_price_list
test_df["PC2_Max_Purchase"] = max_price_list
test_df["PC2_Mean_Purchase"] = mean_price_list
test_df["PC2_25Per_Purchase"] = twentyfive_price_list
test_df["PC2_50Per_Purchase"] = fifty_price_list
test_df["PC2_75Per_Purchase"] = seventyfive_price_list

min_price_list, max_price_list, mean_price_list, twentyfive_price_list,fifty_price_list, seventyfive_price_list = getPurchaseStats(test_df,train_df, "Product_Category_3")
test_df["PC3_Min_Purchase"] = min_price_list
test_df["PC3_Max_Purchase"] = max_price_list
test_df["PC3_Mean_Purchase"] = mean_price_list
test_df["PC3_25Per_Purchase"] = twentyfive_price_list
test_df["PC3_50Per_Purchase"] = fifty_price_list
test_df["PC3_75Per_Purchase"] = seventyfive_price_list

test_df.to_csv(r'E:\Data hackatons\AV black friday sales\test_full_feature.csv',index=False)
#test_df = pd.read_csv(r'E:\Data hackatons\AV black friday sales\test_full_feature.csv')

#define test data
test_data = test_df.drop(columns=['User_ID','Product_ID'],axis=1)

test_data_selection = selection.transform(test_data)

#predict the test data
test_df["Purchase"] = xgb_selected_features.predict(test_data_selection)

IDcol = ['User_ID','Product_ID']
IDcol.append("Purchase")
submission = pd.DataFrame({ x: test_df[x] for x in IDcol})
submission.to_csv(r"E:\Data hackatons\AV black friday sales\submission_xgb1.csv", index=False)








32. Based on the results above, I will go with corr>0.05. Now we will do some hyperparameter tuning of our RandomForestRegressor
I will do Randomized Search CV to find the best parameters

#print the existing parameters
from pprint import pprint
print('Parameters currently in use:\n')
pprint(RF.get_params())

from sklearn.model_selection import RandomizedSearchCV
# Number of trees in random forest
n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]
# Number of features to consider at every split
max_features = ['auto', 'sqrt']
# Maximum number of levels in tree
max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]
max_depth.append(None)
# Minimum number of samples required to split a node
min_samples_split = [2, 5, 10]
# Minimum number of samples required at each leaf node
min_samples_leaf = [1, 2, 4]
# Method of selecting samples for training each tree
bootstrap = [True, False]

# Create the random grid
random_grid = {'n_estimators': n_estimators, 'max_features': max_features,'max_depth': max_depth,'min_samples_split': min_samples_split,'min_samples_leaf': min_samples_leaf, 'bootstrap': bootstrap}

#have a look at the random grid
pprint(random_grid)

# Use the random grid to search for best hyperparameters
# First create the base model to tune
RF = RandomForestRegressor()
# Random search of parameters, using 3 fold cross validation, 
# search across 100 different combinations, and use all available cores
RF_random = RandomizedSearchCV(estimator = RF, param_distributions = random_grid, n_iter = 100, cv = 3, verbose=2, random_state=42)
# Fit the random search model
RF_random.fit(X_train,y_train)

RF_random.best_params_
